{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Prompts: Comprehensive Prompt Engineering\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Prompt engineering** is the art and science of crafting effective inputs for language models. LangChain provides powerful tools to create reusable, composable prompt templates.\n",
    "\n",
    "### What Are Prompt Templates?\n",
    "\n",
    "Prompt templates allow you to:\n",
    "- **Separate logic from content**: Keep prompts reusable and maintainable\n",
    "- **Use variables**: Dynamic prompts with placeholders\n",
    "- **Compose prompts**: Build complex prompts from smaller pieces\n",
    "- **Format consistently**: System, Human, AI messages properly structured\n",
    "- **Parse outputs**: Extract structured data from LLM responses\n",
    "\n",
    "### Why Prompt Templates?\n",
    "\n",
    "| ‚ùå Without Templates | ‚úÖ With Templates |\n",
    "|---------------------|------------------|\n",
    "| f\"Translate {text} to {lang}\" | template.format(text=text, lang=lang) |\n",
    "| Hardcoded strings | Reusable components |\n",
    "| No validation | Type-checked inputs |\n",
    "| Manual message formatting | Automatic ChatMessage creation |\n",
    "\n",
    "---\n",
    "\n",
    "## Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API Key: \")\n",
    "\n",
    "print(\"API key configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 1: Basic ChatPromptTemplate\n",
    "\n",
    "The most common prompt template type for chat models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a simple template\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "# Inspect what it creates\n",
    "formatted = prompt.format(topic=\"programming\")\n",
    "print(\"Formatted prompt:\")\n",
    "print(formatted)\n",
    "\n",
    "# Use in a chain\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4\") | StrOutputParser()\n",
    "result = chain.invoke({\"topic\": \"Python\"})\n",
    "print(f\"\\nResult:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 2: System + Human Messages\n",
    "\n",
    "Create structured prompts with system instructions and user input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a template with multiple message types\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful {role} assistant with expertise in {domain}.\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])\n",
    "\n",
    "# See the template structure\n",
    "print(\"Template structure:\")\n",
    "print(prompt)\n",
    "\n",
    "# Format with variables\n",
    "messages = prompt.format_messages(\n",
    "    role=\"Python programming\",\n",
    "    domain=\"software design patterns\",\n",
    "    user_input=\"Explain the factory pattern\"\n",
    ")\n",
    "\n",
    "print(\"\\nFormatted messages:\")\n",
    "for msg in messages:\n",
    "    print(f\"{msg.__class__.__name__}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 3: Multiple Message Types\n",
    "\n",
    "Include examples of previous conversations (few-shot learning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Template with examples\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a sentiment classifier. Respond with only: positive, negative, or neutral\"),\n",
    "    (\"human\", \"I love this product!\"),\n",
    "    (\"ai\", \"positive\"),\n",
    "    (\"human\", \"This is terrible.\"),\n",
    "    (\"ai\", \"negative\"),\n",
    "    (\"human\", \"It's okay.\"),\n",
    "    (\"ai\", \"neutral\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4\", temperature=0) | StrOutputParser()\n",
    "\n",
    "# Test with new examples\n",
    "test_texts = [\n",
    "    \"This is amazing!\",\n",
    "    \"I hate waiting in lines.\",\n",
    "    \"The weather is nice today.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    sentiment = chain.invoke({\"text\": text})\n",
    "    print(f\"{text} -> {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 4: MessagesPlaceholder\n",
    "\n",
    "For dynamic conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Template with placeholder for chat history\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer based on conversation context.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])\n",
    "\n",
    "# Create conversation history\n",
    "history = [\n",
    "    HumanMessage(content=\"My name is Alice\"),\n",
    "    AIMessage(content=\"Hello Alice! How can I help you today?\"),\n",
    "    HumanMessage(content=\"I like Python programming\"),\n",
    "    AIMessage(content=\"That's great! Python is a versatile language.\")\n",
    "]\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Ask question that requires context\n",
    "response = chain.invoke({\n",
    "    \"chat_history\": history,\n",
    "    \"user_input\": \"What's my name and what do I like?\"\n",
    "})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 5: Few-Shot Prompting\n",
    "\n",
    "Provide examples to guide the model's behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define examples\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"hot\", \"output\": \"cold\"},\n",
    "    {\"input\": \"fast\", \"output\": \"slow\"}\n",
    "]\n",
    "\n",
    "# Create example template\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\")\n",
    "])\n",
    "\n",
    "# Create few-shot template\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "# Create final prompt\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an antonym generator. Respond with only the antonym.\"),\n",
    "    few_shot_prompt,\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create chain\n",
    "chain = final_prompt | ChatOpenAI(model=\"gpt-4\", temperature=0) | StrOutputParser()\n",
    "\n",
    "# Test it\n",
    "result = chain.invoke({\"input\": \"big\"})\n",
    "print(f\"big -> {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 6: Output Parsers - StrOutputParser\n",
    "\n",
    "Extract string content from responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me about {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Without output parser - returns AIMessage object\n",
    "chain_without_parser = prompt | model\n",
    "result = chain_without_parser.invoke({\"topic\": \"Python\"})\n",
    "print(\"Without parser:\")\n",
    "print(f\"Type: {type(result)}\")\n",
    "print(f\"Content: {result.content[:100]}...\\n\")\n",
    "\n",
    "# With output parser - returns string\n",
    "chain_with_parser = prompt | model | StrOutputParser()\n",
    "result = chain_with_parser.invoke({\"topic\": \"Python\"})\n",
    "print(\"With parser:\")\n",
    "print(f\"Type: {type(result)}\")\n",
    "print(f\"Content: {result[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 7: JsonOutputParser\n",
    "\n",
    "Parse JSON from LLM responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Create parser\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# Create prompt with format instructions\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Extract information about {person}.\\n{format_instructions}\\n\"\n",
    "    \"Return JSON with keys: name, occupation, nationality\"\n",
    ")\n",
    "\n",
    "# Add format instructions to prompt\n",
    "prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4\") | parser\n",
    "\n",
    "# Invoke\n",
    "result = chain.invoke({\"person\": \"Marie Curie\"})\n",
    "print(\"Result:\")\n",
    "print(result)\n",
    "print(f\"\\nType: {type(result)}\")\n",
    "print(f\"Name: {result['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 8: PydanticOutputParser\n",
    "\n",
    "Parse into validated Pydantic models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from typing import List\n",
    "\n",
    "# Define output schema\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"Person's full name\")\n",
    "    age: int = Field(description=\"Person's age in years\")\n",
    "    occupation: str = Field(description=\"Person's job or profession\")\n",
    "    skills: List[str] = Field(description=\"List of skills\")\n",
    "    \n",
    "    @validator('age')\n",
    "    def age_must_be_positive(cls, v):\n",
    "        if v <= 0:\n",
    "            raise ValueError('Age must be positive')\n",
    "        return v\n",
    "\n",
    "# Create parser\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "\n",
    "# Create prompt\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate information about a fictional {profession}.\\n{format_instructions}\"\n",
    ")\n",
    "\n",
    "# Add format instructions\n",
    "prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4\") | parser\n",
    "\n",
    "# Invoke\n",
    "result = chain.invoke({\"profession\": \"software engineer\"})\n",
    "print(\"Result:\")\n",
    "print(f\"Type: {type(result)}\")\n",
    "print(f\"Name: {result.name}\")\n",
    "print(f\"Age: {result.age}\")\n",
    "print(f\"Occupation: {result.occupation}\")\n",
    "print(f\"Skills: {', '.join(result.skills)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 9: Partial Prompts\n",
    "\n",
    "Pre-fill some variables, provide others later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from datetime import datetime\n",
    "\n",
    "# Create template with multiple variables\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a {role} assistant. Today is {date}.\\n\\nUser question: {question}\"\n",
    ")\n",
    "\n",
    "# Partially fill the template\n",
    "partial_prompt = prompt.partial(\n",
    "    role=\"Python programming\",\n",
    "    date=datetime.now().strftime(\"%Y-%m-%d\")\n",
    ")\n",
    "\n",
    "# Create chain (role and date are already filled)\n",
    "chain = partial_prompt | ChatOpenAI(model=\"gpt-4\") | StrOutputParser()\n",
    "\n",
    "# Only need to provide 'question' now\n",
    "result = chain.invoke({\"question\": \"What are decorators?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 10: Partial with Functions\n",
    "\n",
    "Use functions to generate dynamic values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to get current time\n",
    "def get_current_time():\n",
    "    return datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "# Create template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Current time: {time}\\n\\nUser: {input}\"\n",
    ")\n",
    "\n",
    "# Partially fill with function\n",
    "partial_prompt = prompt.partial(time=get_current_time)\n",
    "\n",
    "# Create chain\n",
    "chain = partial_prompt | ChatOpenAI(model=\"gpt-4\") | StrOutputParser()\n",
    "\n",
    "# The time will be evaluated at invocation\n",
    "print(\"First call:\")\n",
    "result1 = chain.invoke({\"input\": \"What time is it?\"})\n",
    "print(result1)\n",
    "\n",
    "import time\n",
    "time.sleep(2)\n",
    "\n",
    "print(\"\\nSecond call (2 seconds later):\")\n",
    "result2 = chain.invoke({\"input\": \"What time is it now?\"})\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 11: Prompt Composition\n",
    "\n",
    "Build complex prompts from smaller pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create reusable prompt components\n",
    "system_template = \"You are an expert in {domain}. Answer questions clearly and concisely.\"\n",
    "context_template = \"Context: {context}\"\n",
    "question_template = \"Question: {question}\"\n",
    "\n",
    "# Compose them\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", context_template + \"\\n\\n\" + question_template)\n",
    "])\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4\") | StrOutputParser()\n",
    "\n",
    "# Use with different domains and contexts\n",
    "result = chain.invoke({\n",
    "    \"domain\": \"Python programming\",\n",
    "    \"context\": \"Python has built-in support for async/await syntax.\",\n",
    "    \"question\": \"What are the benefits of using async/await?\"\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 12: Pipeline Prompts\n",
    "\n",
    "Use one prompt's output as another's input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# First prompt: Generate a topic\n",
    "topic_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate a creative topic related to {category}\"\n",
    ")\n",
    "\n",
    "# Second prompt: Write about that topic\n",
    "writing_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a short paragraph about: {topic}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Create two-stage chain\n",
    "topic_chain = topic_prompt | model | parser\n",
    "writing_chain = writing_prompt | model | parser\n",
    "\n",
    "# Execute stages\n",
    "topic = topic_chain.invoke({\"category\": \"artificial intelligence\"})\n",
    "print(f\"Generated topic: {topic}\\n\")\n",
    "\n",
    "paragraph = writing_chain.invoke({\"topic\": topic})\n",
    "print(f\"Paragraph:\\n{paragraph}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 13: Custom Output Parser\n",
    "\n",
    "Create your own parser for specific formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from typing import List\n",
    "\n",
    "# Custom parser for comma-separated lists\n",
    "class CommaSeparatedListParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Parse comma-separated list.\"\"\"\n",
    "    \n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return [item.strip() for item in text.split(\",\")]\n",
    "    \n",
    "    def get_format_instructions(self) -> str:\n",
    "        return \"Your response should be a comma-separated list, like: item1, item2, item3\"\n",
    "\n",
    "# Create parser instance\n",
    "parser = CommaSeparatedListParser()\n",
    "\n",
    "# Create prompt\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"List 5 {category}.\\n{format_instructions}\"\n",
    ")\n",
    "\n",
    "prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4\", temperature=0.7) | parser\n",
    "\n",
    "# Invoke\n",
    "result = chain.invoke({\"category\": \"programming languages\"})\n",
    "print(f\"Type: {type(result)}\")\n",
    "print(f\"Items: {result}\")\n",
    "print(f\"Count: {len(result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 14: Structured Output with JSON Mode\n",
    "\n",
    "Force OpenAI models to return valid JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import json\n",
    "\n",
    "# Create prompt that explicitly asks for JSON\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Extract structured information about {topic}.\\n\"\n",
    "    \"Return JSON with keys: name, description, examples (list), difficulty (beginner/intermediate/advanced)\"\n",
    ")\n",
    "\n",
    "# Enable JSON mode (OpenAI specific)\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    model_kwargs={\"response_format\": {\"type\": \"json_object\"}}\n",
    ")\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# Invoke\n",
    "result = chain.invoke({\"topic\": \"Python decorators\"})\n",
    "print(\"Parsed result:\")\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### ‚úÖ Do\n",
    "\n",
    "1. **Use ChatPromptTemplate** for chat models (not PromptTemplate)\n",
    "2. **Provide clear system messages** to set behavior\n",
    "3. **Use few-shot examples** for complex tasks\n",
    "4. **Validate outputs** with Pydantic parsers\n",
    "5. **Use partial prompts** for dynamic values\n",
    "6. **Compose prompts** from reusable components\n",
    "7. **Include format instructions** from parsers\n",
    "\n",
    "### ‚ùå Don't\n",
    "\n",
    "1. **Don't use f-strings** directly (use templates for validation)\n",
    "2. **Don't forget to handle parsing errors**\n",
    "3. **Don't over-complicate prompts** (keep them focused)\n",
    "4. **Don't hardcode examples** (use variables)\n",
    "5. **Don't ignore format instructions** from parsers\n",
    "\n",
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "### ‚ùå Mistake 1: Wrong Parser for Chat Models\n",
    "\n",
    "```python\n",
    "# Wrong - PromptTemplate for chat model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(\"Hello {name}\")\n",
    "```\n",
    "\n",
    "**Solution**: Use `ChatPromptTemplate` for chat models.\n",
    "\n",
    "### ‚ùå Mistake 2: Missing Variables\n",
    "\n",
    "```python\n",
    "# Template has {topic} but we don't provide it\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me about {topic}\")\n",
    "prompt.invoke({})  # Error!\n",
    "```\n",
    "\n",
    "**Solution**: Always provide all required variables.\n",
    "\n",
    "### ‚ùå Mistake 3: Not Using Format Instructions\n",
    "\n",
    "```python\n",
    "# Parser expects specific format but prompt doesn't specify it\n",
    "parser = JsonOutputParser()\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me about {topic}\")\n",
    "```\n",
    "\n",
    "**Solution**: Use `parser.get_format_instructions()` in your prompt.\n",
    "\n",
    "---\n",
    "\n",
    "## Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Create a translation chain with language detection\n",
    "# Input: text to translate, target language\n",
    "# Output: Pydantic model with source_language, target_language, translated_text\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Your code here:\n",
    "class Translation(BaseModel):\n",
    "    source_language: str = Field(description=\"Detected source language\")\n",
    "    target_language: str = Field(description=\"Target language\")\n",
    "    translated_text: str = Field(description=\"Translated text\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Translation)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following text to {target_language}.\\n\"\n",
    "    \"Text: {text}\\n\\n\"\n",
    "    \"{format_instructions}\"\n",
    ")\n",
    "\n",
    "prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4\") | parser\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"text\": \"Hello, how are you?\",\n",
    "    \"target_language\": \"Spanish\"\n",
    "})\n",
    "\n",
    "print(f\"Source: {result.source_language}\")\n",
    "print(f\"Target: {result.target_language}\")\n",
    "print(f\"Translation: {result.translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Create a code review chain\n",
    "# Input: code snippet\n",
    "# Output: Structured feedback with issues (list), suggestions (list), rating (1-10)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Your code here:\n",
    "class CodeReview(BaseModel):\n",
    "    issues: List[str] = Field(description=\"List of issues found\")\n",
    "    suggestions: List[str] = Field(description=\"List of improvement suggestions\")\n",
    "    rating: int = Field(description=\"Code quality rating from 1-10\", ge=1, le=10)\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=CodeReview)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Review this Python code and provide detailed feedback:\\n\\n\"\n",
    "    \"```python\\n{code}\\n```\\n\\n\"\n",
    "    \"{format_instructions}\"\n",
    ")\n",
    "\n",
    "prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4\") | parser\n",
    "\n",
    "code_sample = \"\"\"\n",
    "def calculate(a, b):\n",
    "    return a + b\n",
    "\"\"\"\n",
    "\n",
    "result = chain.invoke({\"code\": code_sample})\n",
    "print(f\"Rating: {result.rating}/10\")\n",
    "print(f\"Issues: {result.issues}\")\n",
    "print(f\"Suggestions: {result.suggestions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Create a multi-stage research chain\n",
    "# Stage 1: Generate 3 research questions about a topic\n",
    "# Stage 2: Answer each question\n",
    "# Stage 3: Synthesize into a summary\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "# Your code here:\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Stage 1: Generate questions\n",
    "questions_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate 3 specific research questions about {topic}.\\n\"\n",
    "    \"Return as JSON array with key 'questions'.\"\n",
    ")\n",
    "questions_chain = questions_prompt | model | JsonOutputParser()\n",
    "\n",
    "# Stage 2: Answer questions\n",
    "answer_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer this question concisely: {question}\"\n",
    ")\n",
    "answer_chain = answer_prompt | model | StrOutputParser()\n",
    "\n",
    "# Stage 3: Synthesize\n",
    "synthesis_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Synthesize these Q&A pairs into a cohesive summary about {topic}:\\n\\n{qa_pairs}\"\n",
    ")\n",
    "synthesis_chain = synthesis_prompt | model | StrOutputParser()\n",
    "\n",
    "# Execute pipeline\n",
    "topic = \"Python async/await\"\n",
    "\n",
    "# Get questions\n",
    "questions_result = questions_chain.invoke({\"topic\": topic})\n",
    "questions = questions_result[\"questions\"]\n",
    "print(\"Questions:\")\n",
    "for i, q in enumerate(questions, 1):\n",
    "    print(f\"{i}. {q}\")\n",
    "\n",
    "# Answer each\n",
    "qa_pairs = []\n",
    "for q in questions:\n",
    "    answer = answer_chain.invoke({\"question\": q})\n",
    "    qa_pairs.append(f\"Q: {q}\\nA: {answer}\")\n",
    "\n",
    "# Synthesize\n",
    "summary = synthesis_chain.invoke({\n",
    "    \"topic\": topic,\n",
    "    \"qa_pairs\": \"\\n\\n\".join(qa_pairs)\n",
    "})\n",
    "\n",
    "print(f\"\\nSummary:\\n{summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### ‚úÖ What We Learned\n",
    "\n",
    "1. **ChatPromptTemplate**: Create reusable, composable prompts\n",
    "2. **Message Types**: System, Human, AI messages for context\n",
    "3. **MessagesPlaceholder**: Dynamic conversation history\n",
    "4. **Few-Shot Prompting**: Guide behavior with examples\n",
    "5. **Output Parsers**: Extract structured data (String, JSON, Pydantic)\n",
    "6. **Partial Prompts**: Pre-fill variables, add others later\n",
    "7. **Prompt Composition**: Build complex prompts from components\n",
    "8. **Format Instructions**: Tell LLM how to format output\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "- **langchain_lcel.ipynb**: Advanced chain composition\n",
    "- **langchain_rag.ipynb**: Use prompts in RAG pipelines\n",
    "- **langchain_agents.ipynb**: Dynamic prompts with agents\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "- [LangChain Prompts Documentation](https://python.langchain.com/docs/modules/model_io/prompts/)\n",
    "- [Output Parsers Documentation](https://python.langchain.com/docs/modules/model_io/output_parsers/)\n",
    "- [Few-Shot Prompting](https://www.promptingguide.ai/techniques/fewshot)\n",
    "\n",
    "---\n",
    "\n",
    "**Next Notebook**: `langchain_lcel.ipynb` - Master LCEL for advanced chain composition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
