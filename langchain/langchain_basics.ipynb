{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Basics: Getting Started\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**LangChain** is a framework for developing applications powered by language models. It provides abstractions and tools to build complex LLM applications through composable components.\n",
    "\n",
    "### What is LangChain?\n",
    "\n",
    "LangChain enables developers to:\n",
    "- **Connect LLMs to external data sources** (RAG - Retrieval Augmented Generation)\n",
    "- **Build agents** that can use tools and make decisions\n",
    "- **Create complex workflows** by chaining together multiple components\n",
    "- **Maintain conversation memory** across interactions\n",
    "- **Switch between providers** (OpenAI, Anthropic, Google) with minimal code changes\n",
    "\n",
    "### Why LangChain in 2025?\n",
    "\n",
    "- **Provider-agnostic**: Write once, switch between OpenAI, Anthropic, Google easily\n",
    "- **Production-ready patterns**: Built-in best practices for RAG, agents, memory\n",
    "- **Modern architecture**: LangGraph for complex workflows, LCEL for composability\n",
    "- **Rich ecosystem**: 100+ integrations with vector stores, tools, APIs\n",
    "\n",
    "### When to Use LangChain\n",
    "\n",
    "| ‚úÖ Use LangChain For | ‚ùå Don't Use For |\n",
    "|---------------------|------------------|\n",
    "| RAG applications | Simple single LLM calls |\n",
    "| Multi-step workflows | Maximum performance critical paths |\n",
    "| Agents with tools | Static predefined responses |\n",
    "| Provider flexibility | When you only use one provider |\n",
    "| Complex state management | Simple scripts |\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "Install the core LangChain package and provider-specific packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 1.0.5\n"
     ]
    }
   ],
   "source": [
    "# Install LangChain core and providers\n",
    "# Run in terminal:\n",
    "# pip install langchain langchain-openai langchain-anthropic langchain-community\n",
    "\n",
    "# Verify installation\n",
    "import langchain\n",
    "print(f\"LangChain version: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "### 1. Runnables: The Foundation\n",
    "\n",
    "Everything in LangChain implements the **Runnable** interface with these methods:\n",
    "\n",
    "```python\n",
    ".invoke(input)      # Synchronous execution\n",
    ".stream(input)      # Streaming results\n",
    ".batch(inputs)      # Batch processing\n",
    ".ainvoke(input)     # Async execution\n",
    "```\n",
    "\n",
    "### 2. Core Components\n",
    "\n",
    "- **Models**: LLMs and Chat Models (OpenAI, Anthropic, Google, etc.)\n",
    "- **Prompts**: Templates for structuring inputs\n",
    "- **Output Parsers**: Extract structured data from LLM responses\n",
    "- **Chains**: Combine components into workflows (using LCEL)\n",
    "- **Agents**: Let LLMs decide which tools to use\n",
    "- **Memory**: Maintain conversation context\n",
    "- **Retrievers**: Fetch relevant documents (for RAG)\n",
    "\n",
    "### 3. LCEL (LangChain Expression Language)\n",
    "\n",
    "Modern way to compose components using the pipe operator `|`:\n",
    "\n",
    "```python\n",
    "chain = prompt | model | output_parser\n",
    "```\n",
    "\n",
    "Similar to Unix pipes - intuitive and composable!\n",
    "\n",
    "---\n",
    "\n",
    "## First Example: \"Hello World\"\n",
    "\n",
    "Let's start with a simple LLM call using different providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set API keys (you'll need at least one)\n",
    "# Option 1: Set in environment\n",
    "# export OPENAI_API_KEY=\"your-key-here\"\n",
    "# export ANTHROPIC_API_KEY=\"your-key-here\"\n",
    "\n",
    "# Option 2: Set in code (for testing only!)\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Simple LLM Call with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a blockchain-based language service platform that utilizes artificial intelligence for automated translation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "\n",
    "# Simple invoke\n",
    "response = llm.invoke(\"Explain LangChain in one sentence\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Same Code, Different Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_anthropic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_anthropic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatAnthropic\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Switch to Anthropic Claude - same interface!\u001b[39;00m\n\u001b[1;32m      4\u001b[0m llm_claude \u001b[38;5;241m=\u001b[39m ChatAnthropic(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaude-sonnet-4-5-20250929\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_anthropic'"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Switch to Anthropic Claude - same interface!\n",
    "llm_claude = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\", temperature=0.7)\n",
    "\n",
    "# Exact same method call\n",
    "response = llm_claude.invoke(\"Explain LangChain in one sentence\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concept: Provider Independence\n",
    "\n",
    "Notice how both providers use the same `.invoke()` method? This is the power of LangChain's abstraction - write once, switch providers easily!\n",
    "\n",
    "---\n",
    "\n",
    "## Example 3: Streaming Responses\n",
    "\n",
    "For better UX, stream responses token-by-token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the realm of codes and algorithms divine,\n",
      "There lies a language, unassuming yet sublime.\n",
      "Python's the name, simplicity its game,\n",
      "Turning chaos to harmony, programming's prime.\n",
      "\n",
      "Less syntax, more action, clear as day,\n",
      "For both the novice and the expert sway.\n",
      "Towards problems complex and solutions unique,\n",
      "With Python at hand, no delay.\n",
      "\n",
      "Libraries aplenty, versatility utmost,\n",
      "Data science, machine learning, or web host.\n",
      "Nestled in its simplicity, a sophisticated beast,\n",
      "Python, the language, feared by most.\n",
      "\n",
      "Oh glorious Python, efficient and breezy,\n",
      "You make the hard tasks seem so easy.\n",
      "More than just a language, a work of art,\n",
      "Striking the perfect balance, between light and artsy. \n",
      "\n",
      "A programmer's mate, in the digital night,\n",
      "Guiding through challenges with your inner light.\n",
      "Python, the unsung hero of our tales,\n",
      "In lines of code, a world we write.\n",
      "\n",
      "--- Done streaming ---\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Stream response chunks\n",
    "for chunk in llm.stream(\"Write a short poem about Python programming\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n--- Done streaming ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 4: Simple Chain with LCEL\n",
    "\n",
    "Let's create our first chain using the pipe operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't programmers like nature?\n",
      "\n",
      "Because it has too many bugs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 1. Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "# 2. Create a model\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# 3. Create an output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 4. Chain them together with the pipe operator!\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# 5. Use the chain\n",
    "result = chain.invoke({\"topic\": \"programming\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "1. **Prompt** receives `{\"topic\": \"programming\"}` and creates a formatted message\n",
    "2. **Model** receives the message and generates a response\n",
    "3. **Output Parser** extracts the string content from the response\n",
    "\n",
    "The pipe operator `|` makes it read like a data flow!\n",
    "\n",
    "---\n",
    "\n",
    "## Example 5: Chat Messages\n",
    "\n",
    "Chat models work with messages (system, human, AI):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Python, you can use the built-in `open()` function to read a file. This function takes two parameters: the name of the file, and the mode for which the file should be opened.\n",
      "\n",
      "The function returns a file object you can use to access the file's contents.\n",
      "\n",
      "`open()` function gives you several modes for opening files:\n",
      "\n",
      "- `'r'` (read): for reading a file (default)\n",
      "- `'w'` (write): for creating a new file and writing to it (will overwrite any existing file with the same name)\n",
      "- `'a'` (append): for appending to an existing file\n",
      "- `'b'` (binary): for binary mode, which could be read (`'rb'`), write (`'wb'`), or append (`'ab'`)\n",
      "- `'t'` (text): for text mode (default, used with `'r'`, `'w'`, and `'a'`)\n",
      "- `'+'`: for updating (reading and writing)\n",
      "\n",
      "Here's an example of how you can read a file:\n",
      "\n",
      "```python\n",
      "# open the file in read mode\n",
      "file = open(\"filename.txt\", \"r\")\n",
      "\n",
      "# read the file\n",
      "content = file.read()\n",
      "\n",
      "# print the content of the file\n",
      "print(content)\n",
      "\n",
      "# it's important to close the file when you're done with it\n",
      "file.close()\n",
      "```\n",
      "\n",
      "For large files, you can also choose to read line by line:\n",
      "\n",
      "```python\n",
      "file = open(\"filename.txt\", \"r\")\n",
      "\n",
      "# read the file line by line\n",
      "for line in file:\n",
      "    print(line)\n",
      "\n",
      "file.close()\n",
      "```\n",
      "\n",
      "It's easy to forget to close the file. To avoid this, you can use the `with` statement, which automatically closes the file when the block of code is done:\n",
      "\n",
      "```python\n",
      "with open(\"filename.txt\", \"r\") as file:\n",
      "    for line in file:\n",
      "        print(line)\n",
      "```\n",
      "\n",
      "This last approach is commonly preferred because it ensures the file is properly closed.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful Python programming assistant.\"),\n",
    "    HumanMessage(content=\"How do I read a file in Python?\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message Types\n",
    "\n",
    "- **SystemMessage**: Instructions for the model's behavior\n",
    "- **HumanMessage**: User input\n",
    "- **AIMessage**: Model's previous responses (for conversation history)\n",
    "\n",
    "---\n",
    "\n",
    "## Example 6: Batch Processing\n",
    "\n",
    "Process multiple inputs efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France: The capital of France is Paris.\n",
      "Japan: The capital of Japan is Tokyo.\n",
      "Brazil: The capital of Brazil is Bras√≠lia.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a chain\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "    | ChatOpenAI(model=\"gpt-4\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Batch process multiple inputs\n",
    "countries = [{\"country\": \"France\"}, {\"country\": \"Japan\"}, {\"country\": \"Brazil\"}]\n",
    "results = chain.batch(countries)\n",
    "\n",
    "for country, capital in zip([c[\"country\"] for c in countries], results):\n",
    "    print(f\"{country}: {capital}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 7: Async Execution\n",
    "\n",
    "For non-blocking operations, use async methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create chain\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_template(\"Tell me about {topic}\")\n",
    "    | ChatOpenAI(model=\"gpt-4\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Async function\n",
    "async def get_info(topic):\n",
    "    result = await chain.ainvoke({\"topic\": topic})\n",
    "    return result\n",
    "\n",
    "# Run multiple requests concurrently\n",
    "async def main():\n",
    "    tasks = [\n",
    "        get_info(\"Python asyncio\"),\n",
    "        get_info(\"Python threading\"),\n",
    "        get_info(\"Python multiprocessing\")\n",
    "    ]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nResult {i}:\\n{result[:100]}...\")\n",
    "\n",
    "# Run in Jupyter\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### ‚úÖ What We Learned\n",
    "\n",
    "1. **Runnable Interface**: All components share `.invoke()`, `.stream()`, `.batch()`, `.ainvoke()`\n",
    "2. **Provider Independence**: Switch between OpenAI, Anthropic, Google with minimal code changes\n",
    "3. **LCEL (Pipe Operator)**: Chain components with `|` for readable workflows\n",
    "4. **Message Types**: System, Human, AI messages for chat models\n",
    "5. **Streaming**: Better UX with token-by-token responses\n",
    "6. **Batch Processing**: Efficient multiple input handling\n",
    "7. **Async Support**: Non-blocking operations for performance\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "- **langchain_models.ipynb**: Deep dive into different model types and providers\n",
    "- **langchain_prompts.ipynb**: Advanced prompt engineering techniques\n",
    "- **langchain_lcel.ipynb**: Master the Expression Language\n",
    "\n",
    "---\n",
    "\n",
    "## Practice Exercises\n",
    "\n",
    "Try these exercises to reinforce your learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Create a chain that translates text to a specified language\n",
    "# Hint: Use ChatPromptTemplate with {text} and {language} variables\n",
    "\n",
    "# Your code here:\n",
    "translation_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Translate the following text to {language}: {text}\")\n",
    "    | ChatOpenAI(model=\"gpt-4\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test it\n",
    "result = translation_chain.invoke({\"text\": \"Hello, how are you?\", \"language\": \"Spanish\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Create a streaming chain that generates a story\n",
    "# Stream the output token by token\n",
    "\n",
    "# Your code here:\n",
    "story_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Write a short story about {topic}\")\n",
    "    | ChatOpenAI(model=\"gpt-4\")\n",
    ")\n",
    "\n",
    "# Stream it\n",
    "for chunk in story_chain.stream({\"topic\": \"a robot learning to code\"}):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Batch process sentiment analysis for multiple texts\n",
    "# Classify each text as positive, negative, or neutral\n",
    "\n",
    "# Your code here:\n",
    "sentiment_chain = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"Classify the sentiment of this text as positive, negative, or neutral. \"\n",
    "        \"Respond with only one word: {text}\"\n",
    "    )\n",
    "    | ChatOpenAI(model=\"gpt-4\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "texts = [\n",
    "    {\"text\": \"I love this product!\"},\n",
    "    {\"text\": \"This is terrible.\"},\n",
    "    {\"text\": \"It's okay, nothing special.\"}\n",
    "]\n",
    "\n",
    "results = sentiment_chain.batch(texts)\n",
    "for text, sentiment in zip(texts, results):\n",
    "    print(f\"{text['text']} -> {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "### ‚ùå Mistake 1: Forgetting to Set API Keys\n",
    "\n",
    "```python\n",
    "# This will fail if OPENAI_API_KEY is not set\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "```\n",
    "\n",
    "**Solution**: Always check environment variables or pass `api_key` explicitly.\n",
    "\n",
    "### ‚ùå Mistake 2: Using Wrong Message Format\n",
    "\n",
    "```python\n",
    "# Wrong - ChatModels expect messages, not strings\n",
    "llm.invoke(\"Hello\")\n",
    "```\n",
    "\n",
    "**Solution**: Use proper message types or let prompts handle formatting.\n",
    "\n",
    "### ‚ùå Mistake 3: Not Handling Rate Limits\n",
    "\n",
    "```python\n",
    "# Batch processing 1000 requests without rate limiting\n",
    "results = chain.batch([{\"input\": i} for i in range(1000)])\n",
    "```\n",
    "\n",
    "**Solution**: Use `max_concurrency` parameter or implement retry logic.\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [LangChain GitHub](https://github.com/langchain-ai/langchain)\n",
    "- [LangSmith](https://smith.langchain.com/) - Debugging and monitoring\n",
    "- [LangChain Blog](https://blog.langchain.dev/)\n",
    "\n",
    "---\n",
    "\n",
    "**Next Notebook**: `langchain_models.ipynb` - Deep dive into LLMs, Chat Models, and providers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
