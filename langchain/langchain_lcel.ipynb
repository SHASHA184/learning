{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL): Advanced Chain Composition\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**LCEL (LangChain Expression Language)** is the modern way to compose LangChain components using declarative syntax. It makes chains readable, composable, and powerful.\n",
    "\n",
    "### What is LCEL?\n",
    "\n",
    "LCEL allows you to:\n",
    "- **Compose chains** with the pipe operator `|`\n",
    "- **Run in parallel** with `RunnableParallel`\n",
    "- **Add conditional logic** with `RunnableBranch`\n",
    "- **Create custom functions** with `RunnableLambda`\n",
    "- **Handle errors gracefully** with fallbacks and retries\n",
    "- **Stream results** automatically\n",
    "\n",
    "### Why LCEL?\n",
    "\n",
    "| ‚ùå Legacy Chains | ‚úÖ LCEL |\n",
    "|-----------------|--------|\n",
    "| `LLMChain(llm=llm, prompt=prompt)` | `prompt \\| model \\| parser` |\n",
    "| Hard to compose | Pipe operator intuitive |\n",
    "| Limited streaming | Built-in streaming |\n",
    "| Verbose syntax | Concise and readable |\n",
    "\n",
    "### Core Concept: Runnables\n",
    "\n",
    "Everything in LCEL is a `Runnable` with these methods:\n",
    "- `.invoke(input)` - Synchronous execution\n",
    "- `.stream(input)` - Streaming results\n",
    "- `.batch(inputs)` - Batch processing\n",
    "- `.ainvoke(input)` - Async execution\n",
    "\n",
    "---\n",
    "\n",
    "## Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API Key: \")\n",
    "\n",
    "print(\"API key configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 1: Basic Pipe Operator\n",
    "\n",
    "The pipe `|` operator chains components together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create components\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Chain with pipe operator (reads like a data flow!)\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# Invoke\n",
    "result = chain.invoke({\"topic\": \"programming\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Happening?\n",
    "\n",
    "1. `prompt` receives `{\"topic\": \"programming\"}` ‚Üí creates formatted message\n",
    "2. `model` receives message ‚Üí generates AIMessage\n",
    "3. `parser` receives AIMessage ‚Üí extracts string\n",
    "\n",
    "The `|` operator automatically passes output from left to right!\n",
    "\n",
    "---\n",
    "\n",
    "## Example 2: RunnablePassthrough\n",
    "\n",
    "Pass input through unchanged (useful for parallel operations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Simple passthrough example\n",
    "passthrough = RunnablePassthrough()\n",
    "result = passthrough.invoke({\"key\": \"value\"})\n",
    "print(f\"Passthrough result: {result}\")\n",
    "\n",
    "# Use in chain to preserve input\n",
    "prompt = ChatPromptTemplate.from_template(\"Explain {concept} in simple terms\")\n",
    "chain = {\"concept\": RunnablePassthrough()} | prompt | ChatOpenAI(model=\"gpt-4\") | StrOutputParser()\n",
    "\n",
    "# Input is just a string, passthrough makes it dict with 'concept' key\n",
    "result = chain.invoke(\"recursion\")\n",
    "print(f\"\\nExplanation:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 3: RunnableParallel - Parallel Execution\n",
    "\n",
    "Run multiple operations in parallel and combine results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Create multiple chains that run in parallel\n",
    "joke_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Tell a joke about {topic}\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "fact_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Tell an interesting fact about {topic}\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "poem_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Write a haiku about {topic}\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run all in parallel\n",
    "parallel_chain = RunnableParallel(\n",
    "    joke=joke_chain,\n",
    "    fact=fact_chain,\n",
    "    poem=poem_chain\n",
    ")\n",
    "\n",
    "# Execute (all three LLM calls happen in parallel!)\n",
    "result = parallel_chain.invoke({\"topic\": \"Python\"})\n",
    "\n",
    "print(\"Joke:\")\n",
    "print(result[\"joke\"])\n",
    "print(\"\\nFact:\")\n",
    "print(result[\"fact\"])\n",
    "print(\"\\nPoem:\")\n",
    "print(result[\"poem\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Benefits\n",
    "\n",
    "- **Sequential**: 3 chains √ó 2 seconds each = 6 seconds total\n",
    "- **Parallel**: max(2, 2, 2) = ~2 seconds total\n",
    "\n",
    "**3x faster!**\n",
    "\n",
    "---\n",
    "\n",
    "## Example 4: Dict Syntax for RunnableParallel\n",
    "\n",
    "Shorthand syntax using dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Dict syntax automatically creates RunnableParallel\n",
    "chain = (\n",
    "    # Pass through input and add uppercase version\n",
    "    {\n",
    "        \"original\": RunnablePassthrough(),\n",
    "        \"uppercase\": lambda x: x[\"word\"].upper(),\n",
    "        \"length\": lambda x: len(x[\"word\"])\n",
    "    }\n",
    "    # These all run in parallel!\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"word\": \"python\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 5: RunnableLambda - Custom Functions\n",
    "\n",
    "Wrap Python functions as Runnables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define custom functions\n",
    "def preprocess_input(input_dict):\n",
    "    \"\"\"Clean and prepare input.\"\"\"\n",
    "    text = input_dict[\"text\"]\n",
    "    return {\"cleaned_text\": text.strip().lower()}\n",
    "\n",
    "def postprocess_output(text):\n",
    "    \"\"\"Format output.\"\"\"\n",
    "    return f\"[PROCESSED] {text.upper()}\"\n",
    "\n",
    "# Wrap as Runnables\n",
    "preprocess = RunnableLambda(preprocess_input)\n",
    "postprocess = RunnableLambda(postprocess_output)\n",
    "\n",
    "# Build chain\n",
    "prompt = ChatPromptTemplate.from_template(\"Explain: {cleaned_text}\")\n",
    "chain = preprocess | prompt | ChatOpenAI(model=\"gpt-4\") | StrOutputParser() | postprocess\n",
    "\n",
    "result = chain.invoke({\"text\": \"  Python Decorators  \"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shorthand: Direct Lambda\n",
    "\n",
    "You can use lambda functions directly without wrapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Lambda directly in chain (auto-wrapped as RunnableLambda)\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_template(\"{number}\")\n",
    "    | ChatOpenAI(model=\"gpt-4\")\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: f\"Result: {x}\")\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"number\": \"What is 2+2?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 6: RunnableBranch - Conditional Logic\n",
    "\n",
    "Route to different chains based on conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Different chains for different languages\n",
    "python_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Explain this Python concept: {topic}\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "javascript_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Explain this JavaScript concept: {topic}\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "general_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Explain this concept: {topic}\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create branching logic\n",
    "branch = RunnableBranch(\n",
    "    # (condition, runnable) pairs\n",
    "    (lambda x: x.get(\"language\") == \"python\", python_chain),\n",
    "    (lambda x: x.get(\"language\") == \"javascript\", javascript_chain),\n",
    "    # Default (no condition)\n",
    "    general_chain\n",
    ")\n",
    "\n",
    "# Test different branches\n",
    "print(\"Python branch:\")\n",
    "result = branch.invoke({\"language\": \"python\", \"topic\": \"decorators\"})\n",
    "print(result[:100] + \"...\\n\")\n",
    "\n",
    "print(\"JavaScript branch:\")\n",
    "result = branch.invoke({\"language\": \"javascript\", \"topic\": \"promises\"})\n",
    "print(result[:100] + \"...\\n\")\n",
    "\n",
    "print(\"Default branch:\")\n",
    "result = branch.invoke({\"language\": \"unknown\", \"topic\": \"algorithms\"})\n",
    "print(result[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 7: Error Handling with Fallbacks\n",
    "\n",
    "Gracefully handle failures by falling back to alternatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me about {topic}\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Primary: Expensive model\n",
    "primary_chain = prompt | ChatOpenAI(model=\"gpt-4\", timeout=1) | parser\n",
    "\n",
    "# Fallback 1: Cheaper model\n",
    "fallback1_chain = prompt | ChatOpenAI(model=\"gpt-3.5-turbo\") | parser\n",
    "\n",
    "# Fallback 2: Simple response\n",
    "fallback2_chain = lambda x: f\"Sorry, I couldn't process your request about {x['topic']}\"\n",
    "\n",
    "# Chain with fallbacks\n",
    "chain_with_fallbacks = primary_chain.with_fallbacks(\n",
    "    [fallback1_chain, fallback2_chain]\n",
    ")\n",
    "\n",
    "# If GPT-4 fails (timeout/error), tries GPT-3.5, then fallback message\n",
    "result = chain_with_fallbacks.invoke({\"topic\": \"Python\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 8: Retry Logic\n",
    "\n",
    "Automatically retry on failures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"What is {topic}?\")\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# Add retry logic (retries up to 3 times on failure)\n",
    "chain_with_retry = chain.with_retry(\n",
    "    stop_after_attempt=3,\n",
    "    wait_exponential_jitter=True  # Exponential backoff with jitter\n",
    ")\n",
    "\n",
    "result = chain_with_retry.invoke({\"topic\": \"async programming\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 9: Streaming with LCEL\n",
    "\n",
    "LCEL chains support streaming by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_template(\"Write a short story about {topic}\")\n",
    "    | ChatOpenAI(model=\"gpt-4\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Stream tokens as they arrive\n",
    "print(\"Streaming story:\\n\")\n",
    "for chunk in chain.stream({\"topic\": \"a robot learning to code\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 10: Batch Processing\n",
    "\n",
    "Process multiple inputs efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "    | ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Batch inputs\n",
    "countries = [\n",
    "    {\"country\": \"France\"},\n",
    "    {\"country\": \"Japan\"},\n",
    "    {\"country\": \"Brazil\"},\n",
    "    {\"country\": \"Egypt\"}\n",
    "]\n",
    "\n",
    "# Process in batch (more efficient than individual calls)\n",
    "results = chain.batch(countries)\n",
    "\n",
    "for country, capital in zip(countries, results):\n",
    "    print(f\"{country['country']}: {capital}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 11: Async Execution\n",
    "\n",
    "Use async methods for concurrent processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_template(\"Explain {concept} in one sentence\")\n",
    "    | ChatOpenAI(model=\"gpt-4\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "async def process_concepts():\n",
    "    concepts = [\"recursion\", \"polymorphism\", \"encapsulation\"]\n",
    "    \n",
    "    # Process concurrently\n",
    "    tasks = [chain.ainvoke({\"concept\": c}) for c in concepts]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    for concept, result in zip(concepts, results):\n",
    "        print(f\"{concept}: {result}\\n\")\n",
    "\n",
    "# Run in Jupyter\n",
    "await process_concepts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 12: Complex Chain - RAG-like Pattern\n",
    "\n",
    "Combine multiple LCEL features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Simulate document retrieval\n",
    "def retrieve_context(query):\n",
    "    \"\"\"Simulate fetching relevant context.\"\"\"\n",
    "    # In real RAG, this would query a vector store\n",
    "    contexts = {\n",
    "        \"What are design patterns?\": \"Design patterns are reusable solutions to common software design problems.\",\n",
    "        \"What is Python?\": \"Python is a high-level, interpreted programming language known for readability.\"\n",
    "    }\n",
    "    return contexts.get(query[\"question\"], \"No context found.\")\n",
    "\n",
    "# Build RAG-like chain\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    # Step 1: Get question and retrieve context in parallel\n",
    "    RunnableParallel(\n",
    "        context=retrieve_context,\n",
    "        question=RunnablePassthrough()\n",
    "    )\n",
    "    # Step 2: Format prompt with context + question\n",
    "    | (lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"][\"question\"]})\n",
    "    # Step 3: Get answer from LLM\n",
    "    | prompt\n",
    "    | ChatOpenAI(model=\"gpt-4\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"question\": \"What are design patterns?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 13: itemgetter for Extracting Keys\n",
    "\n",
    "Use `itemgetter` to extract specific keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Chain that uses only specific keys\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate '{text}' to {language}\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    # Extract only needed keys from input dict\n",
    "    {\n",
    "        \"text\": itemgetter(\"text\"),\n",
    "        \"language\": itemgetter(\"target_language\")\n",
    "    }\n",
    "    | prompt\n",
    "    | ChatOpenAI(model=\"gpt-4\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Input has extra keys that are ignored\n",
    "result = chain.invoke({\n",
    "    \"text\": \"Hello\",\n",
    "    \"target_language\": \"Spanish\",\n",
    "    \"extra_key\": \"ignored\"\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 14: Multi-Step Chain with State\n",
    "\n",
    "Build chains that pass state between steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Step 1: Generate outline\n",
    "outline_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Create a 3-point outline for: {topic}\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Step 2: Expand outline\n",
    "expand_chain = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"Topic: {topic}\\nOutline: {outline}\\n\\nExpand this outline into a full explanation:\"\n",
    "    )\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Combine steps while preserving topic\n",
    "full_chain = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | RunnablePassthrough.assign(outline=outline_chain)\n",
    "    | expand_chain\n",
    ")\n",
    "\n",
    "result = full_chain.invoke({\"topic\": \"Python asyncio\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Real-World Example: Code Review Chain\n",
    "\n",
    "Complete example combining many LCEL features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Parallel analysis chains\n",
    "bugs_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Find potential bugs in this code. Return JSON with 'bugs' list:\\n{code}\"\n",
    ")\n",
    "bugs_chain = bugs_prompt | model | JsonOutputParser()\n",
    "\n",
    "style_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Analyze code style. Return JSON with 'issues' list:\\n{code}\"\n",
    ")\n",
    "style_chain = style_prompt | model | JsonOutputParser()\n",
    "\n",
    "performance_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Analyze performance. Return JSON with 'suggestions' list:\\n{code}\"\n",
    ")\n",
    "performance_chain = performance_prompt | model | JsonOutputParser()\n",
    "\n",
    "# Run all analyses in parallel\n",
    "analysis_chain = RunnableParallel(\n",
    "    code=RunnablePassthrough(),\n",
    "    bugs=bugs_chain,\n",
    "    style=style_chain,\n",
    "    performance=performance_chain\n",
    ")\n",
    "\n",
    "# Synthesis chain\n",
    "synthesis_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Code:\\n{code}\\n\\nBugs: {bugs}\\nStyle: {style}\\nPerformance: {performance}\\n\\n\"\n",
    "    \"Provide a summary code review:\"\n",
    ")\n",
    "synthesis_chain = synthesis_prompt | model | StrOutputParser()\n",
    "\n",
    "# Full chain\n",
    "code_review_chain = analysis_chain | synthesis_chain\n",
    "\n",
    "# Test it\n",
    "code_sample = \"\"\"\n",
    "def calculate_sum(numbers):\n",
    "    total = 0\n",
    "    for i in range(len(numbers)):\n",
    "        total = total + numbers[i]\n",
    "    return total\n",
    "\"\"\"\n",
    "\n",
    "result = code_review_chain.invoke({\"code\": code_sample})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### ‚úÖ Do\n",
    "\n",
    "1. **Use pipe operator** for sequential operations\n",
    "2. **Use RunnableParallel** for independent operations\n",
    "3. **Use RunnableBranch** for conditional logic\n",
    "4. **Add fallbacks** for production reliability\n",
    "5. **Use RunnablePassthrough** to preserve input\n",
    "6. **Leverage streaming** for better UX\n",
    "7. **Use async** for I/O-bound operations\n",
    "\n",
    "### ‚ùå Don't\n",
    "\n",
    "1. **Don't use legacy chains** (LLMChain, etc.)\n",
    "2. **Don't run independent operations sequentially** (use parallel)\n",
    "3. **Don't ignore errors** (add fallbacks/retries)\n",
    "4. **Don't make chains too complex** (break into smaller pieces)\n",
    "5. **Don't forget type hints** (helps with debugging)\n",
    "\n",
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "### ‚ùå Mistake 1: Not Preserving Input\n",
    "\n",
    "```python\n",
    "# Wrong - loses original input\n",
    "chain = transform | prompt | model\n",
    "```\n",
    "\n",
    "**Solution**: Use RunnablePassthrough to preserve:\n",
    "```python\n",
    "chain = {\"original\": RunnablePassthrough(), \"transformed\": transform} | ...\n",
    "```\n",
    "\n",
    "### ‚ùå Mistake 2: Sequential Instead of Parallel\n",
    "\n",
    "```python\n",
    "# Slow - runs sequentially\n",
    "result1 = chain1.invoke(input)\n",
    "result2 = chain2.invoke(input)\n",
    "```\n",
    "\n",
    "**Solution**: Use RunnableParallel:\n",
    "```python\n",
    "parallel = RunnableParallel(r1=chain1, r2=chain2)\n",
    "results = parallel.invoke(input)\n",
    "```\n",
    "\n",
    "### ‚ùå Mistake 3: No Error Handling\n",
    "\n",
    "```python\n",
    "# Fragile - fails completely on any error\n",
    "chain = prompt | expensive_model | parser\n",
    "```\n",
    "\n",
    "**Solution**: Add fallbacks:\n",
    "```python\n",
    "chain = (prompt | expensive_model | parser).with_fallbacks([cheap_model_chain])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Create a research chain\n",
    "# 1. Generate 3 questions about a topic\n",
    "# 2. Answer each question in parallel\n",
    "# 3. Synthesize answers into summary\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Your code here:\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Step 1: Generate questions\n",
    "questions_chain = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"Generate 3 questions about {topic}. Return JSON with 'questions' array.\"\n",
    "    )\n",
    "    | model\n",
    "    | JsonOutputParser()\n",
    ")\n",
    "\n",
    "# Step 2: Answer questions (you'll need to implement this)\n",
    "# Step 3: Synthesize (you'll need to implement this)\n",
    "\n",
    "# Test your chain\n",
    "# result = research_chain.invoke({\"topic\": \"Python decorators\"})\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Create a sentiment analysis chain with fallback\n",
    "# - Try using expensive model first\n",
    "# - Fall back to simple keyword matching if it fails\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Your code here:\n",
    "def keyword_sentiment(input_dict):\n",
    "    \"\"\"Simple keyword-based sentiment.\"\"\"\n",
    "    text = input_dict[\"text\"].lower()\n",
    "    if any(word in text for word in [\"love\", \"great\", \"amazing\"]):\n",
    "        return \"positive\"\n",
    "    elif any(word in text for word in [\"hate\", \"terrible\", \"awful\"]):\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "# Create chain with fallback\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Create a multi-language translation chain\n",
    "# - Detect source language\n",
    "# - Translate to target language\n",
    "# - Run in parallel: translate to Spanish, French, German\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Your code here:\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Create chains for each language\n",
    "# Combine with RunnableParallel\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### ‚úÖ What We Learned\n",
    "\n",
    "1. **Pipe Operator `|`**: Chain components sequentially\n",
    "2. **RunnablePassthrough**: Preserve input through chain\n",
    "3. **RunnableParallel**: Execute independent operations concurrently\n",
    "4. **RunnableBranch**: Conditional routing to different chains\n",
    "5. **RunnableLambda**: Wrap Python functions as Runnables\n",
    "6. **Error Handling**: Fallbacks and retries for reliability\n",
    "7. **Streaming**: Built-in support for token-by-token output\n",
    "8. **Batch/Async**: Efficient processing of multiple inputs\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "- **langchain_rag.ipynb**: Apply LCEL to RAG pipelines\n",
    "- **langchain_agents.ipynb**: Use LCEL with agents\n",
    "- **langchain_memory.ipynb**: Add memory to LCEL chains\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [LCEL Documentation](https://python.langchain.com/docs/expression_language/)\n",
    "- [Runnable Interface](https://python.langchain.com/docs/expression_language/interface)\n",
    "- [LCEL Cookbook](https://python.langchain.com/docs/expression_language/cookbook)\n",
    "- [Streaming Guide](https://python.langchain.com/docs/expression_language/streaming)\n",
    "\n",
    "---\n",
    "\n",
    "**Next Notebook**: `langchain_rag.ipynb` - Build complete RAG applications with LCEL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
