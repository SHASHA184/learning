{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Memory: Conversation Memory Patterns\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Memory** allows LLM applications to remember conversation history and maintain context across multiple interactions. Essential for chatbots and multi-turn conversations.\n",
    "\n",
    "### What is Memory?\n",
    "\n",
    "Memory enables:\n",
    "- **Conversation continuity**: Remember what was said before\n",
    "- **Context awareness**: Understand references (\"it\", \"that\", \"the previous answer\")\n",
    "- **Personalization**: Remember user preferences\n",
    "- **Multi-turn reasoning**: Build on previous exchanges\n",
    "- **State management**: Track conversation state\n",
    "\n",
    "### Memory vs Stateless\n",
    "\n",
    "| Stateless (No Memory) | With Memory |\n",
    "|----------------------|-------------|\n",
    "| Each call independent | Remembers history |\n",
    "| No context awareness | Contextual responses |\n",
    "| Can't reference past | Can reference past |\n",
    "| Simpler | More complex |\n",
    "\n",
    "### When to Use Memory?\n",
    "\n",
    "| ‚úÖ Use Memory For | ‚ùå Don't Use For |\n",
    "|-------------------|------------------|\n",
    "| Chatbots | Single Q&A |\n",
    "| Multi-turn conversations | Independent requests |\n",
    "| Context-dependent tasks | Stateless APIs |\n",
    "| Personalized interactions | Batch processing |\n",
    "\n",
    "---\n",
    "\n",
    "## Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API Key: \")\n",
    "\n",
    "print(\"API key configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 1: No Memory (Baseline)\n",
    "\n",
    "See the problem without memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Simple chain without memory\n",
    "prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# First message\n",
    "response1 = chain.invoke({\"input\": \"My name is Alice\"})\n",
    "print(f\"User: My name is Alice\")\n",
    "print(f\"AI: {response1}\\n\")\n",
    "\n",
    "# Second message - no memory!\n",
    "response2 = chain.invoke({\"input\": \"What's my name?\"})\n",
    "print(f\"User: What's my name?\")\n",
    "print(f\"AI: {response2}\")\n",
    "print(\"\\n‚ùå Problem: AI doesn't remember the conversation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 2: ConversationBufferMemory (Legacy)\n",
    "\n",
    "Store all messages in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Create memory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# Create prompt with history placeholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Helper function to manage memory\n",
    "def chat(user_input: str) -> str:\n",
    "    # Get history from memory\n",
    "    history = memory.load_memory_variables({})[\"history\"]\n",
    "    \n",
    "    # Create chain\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # Get response\n",
    "    response = chain.invoke({\"history\": history, \"input\": user_input})\n",
    "    \n",
    "    # Save to memory\n",
    "    memory.save_context({\"input\": user_input}, {\"output\": response})\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Conversation with memory\n",
    "print(\"User: My name is Alice\")\n",
    "response1 = chat(\"My name is Alice\")\n",
    "print(f\"AI: {response1}\\n\")\n",
    "\n",
    "print(\"User: What's my name?\")\n",
    "response2 = chat(\"What's my name?\")\n",
    "print(f\"AI: {response2}\")\n",
    "print(\"\\n‚úÖ Success: AI remembers the conversation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 3: RunnableWithMessageHistory (Modern Approach)\n",
    "\n",
    "The recommended way to add memory in 2025:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Create prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create chain\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Create message history store\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Wrap chain with message history\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# Configuration for session\n",
    "config = {\"configurable\": {\"session_id\": \"user_alice\"}}\n",
    "\n",
    "# Conversation\n",
    "print(\"User: My name is Alice and I love Python\")\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"input\": \"My name is Alice and I love Python\"},\n",
    "    config=config\n",
    ")\n",
    "print(f\"AI: {response1}\\n\")\n",
    "\n",
    "print(\"User: What's my name?\")\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"input\": \"What's my name?\"},\n",
    "    config=config\n",
    ")\n",
    "print(f\"AI: {response2}\\n\")\n",
    "\n",
    "print(\"User: What programming language do I like?\")\n",
    "response3 = chain_with_history.invoke(\n",
    "    {\"input\": \"What programming language do I like?\"},\n",
    "    config=config\n",
    ")\n",
    "print(f\"AI: {response3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 4: Multiple Sessions\n",
    "\n",
    "Manage separate conversations for different users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Setup (same as before)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4\") | StrOutputParser()\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# User 1 conversation\n",
    "config_alice = {\"configurable\": {\"session_id\": \"alice\"}}\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"My favorite color is blue\"},\n",
    "    config=config_alice\n",
    ")\n",
    "print(f\"Alice: My favorite color is blue\")\n",
    "print(f\"AI: {response}\\n\")\n",
    "\n",
    "# User 2 conversation (different session)\n",
    "config_bob = {\"configurable\": {\"session_id\": \"bob\"}}\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"My favorite color is red\"},\n",
    "    config=config_bob\n",
    ")\n",
    "print(f\"Bob: My favorite color is red\")\n",
    "print(f\"AI: {response}\\n\")\n",
    "\n",
    "# Check Alice's memory\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"What's my favorite color?\"},\n",
    "    config=config_alice\n",
    ")\n",
    "print(f\"Alice: What's my favorite color?\")\n",
    "print(f\"AI: {response}\\n\")\n",
    "\n",
    "# Check Bob's memory\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"What's my favorite color?\"},\n",
    "    config=config_bob\n",
    ")\n",
    "print(f\"Bob: What's my favorite color?\")\n",
    "print(f\"AI: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 5: ConversationBufferWindowMemory\n",
    "\n",
    "Keep only the last N messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Keep only last 2 interactions (4 messages)\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=2,  # Keep last 2 exchanges\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "def chat(user_input: str) -> str:\n",
    "    history = memory.load_memory_variables({})[\"history\"]\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({\"history\": history, \"input\": user_input})\n",
    "    memory.save_context({\"input\": user_input}, {\"output\": response})\n",
    "    return response\n",
    "\n",
    "# Multiple messages\n",
    "messages = [\n",
    "    \"My name is Alice\",\n",
    "    \"I live in New York\",\n",
    "    \"I like Python programming\",\n",
    "    \"What's my name?\"  # This should work (within window)\n",
    "]\n",
    "\n",
    "for msg in messages:\n",
    "    response = chat(msg)\n",
    "    print(f\"User: {msg}\")\n",
    "    print(f\"AI: {response}\\n\")\n",
    "\n",
    "# This might not remember name (outside window)\n",
    "response = chat(\"What did I tell you first?\")\n",
    "print(f\"User: What did I tell you first?\")\n",
    "print(f\"AI: {response}\")\n",
    "print(\"\\n‚ö†Ô∏è May not remember early messages (outside window)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 6: ConversationSummaryMemory\n",
    "\n",
    "Summarize conversation to save tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create summary memory\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Simulate conversation\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hi! My name is Alice and I'm a software engineer.\"},\n",
    "    {\"output\": \"Hello Alice! Nice to meet you. How can I help you today?\"}\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"I work on Python backend services and I love design patterns.\"},\n",
    "    {\"output\": \"That's great! Design patterns are very useful in backend development.\"}\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"I'm particularly interested in the factory and observer patterns.\"},\n",
    "    {\"output\": \"Those are excellent choices for backend systems!\"}\n",
    ")\n",
    "\n",
    "# Get summarized history\n",
    "history = memory.load_memory_variables({})\n",
    "print(\"Summarized conversation:\")\n",
    "print(history[\"history\"])\n",
    "print(\"\\n‚úÖ Summary preserves key facts while reducing tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 7: ConversationSummaryBufferMemory\n",
    "\n",
    "Hybrid: Keep recent messages + summary of old ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Keep recent messages + summarize old ones\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=100,  # When to start summarizing\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Add many messages\n",
    "conversations = [\n",
    "    ({\"input\": \"My name is Alice\"}, {\"output\": \"Nice to meet you, Alice!\"}),\n",
    "    ({\"input\": \"I'm a Python developer\"}, {\"output\": \"That's great!\"}),\n",
    "    ({\"input\": \"I work at a tech company\"}, {\"output\": \"Interesting!\"}),\n",
    "    ({\"input\": \"I love building APIs\"}, {\"output\": \"APIs are fundamental!\"}),\n",
    "]\n",
    "\n",
    "for inp, out in conversations:\n",
    "    memory.save_context(inp, out)\n",
    "\n",
    "# Check memory (should have summary + recent messages)\n",
    "history = memory.load_memory_variables({})\n",
    "print(\"Memory contents:\")\n",
    "print(history[\"history\"])\n",
    "print(\"\\n‚úÖ Balances detail (recent) with efficiency (summary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 8: Persistent Memory with SQLite\n",
    "\n",
    "Store conversation history in database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Create prompt and chain\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4\") | StrOutputParser()\n",
    "\n",
    "# Function to get SQL-backed message history\n",
    "def get_session_history(session_id: str):\n",
    "    return SQLChatMessageHistory(\n",
    "        session_id=session_id,\n",
    "        connection_string=\"sqlite:///chat_history.db\"\n",
    "    )\n",
    "\n",
    "# Wrap with history\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# Conversation (persisted to SQLite)\n",
    "config = {\"configurable\": {\"session_id\": \"user_123\"}}\n",
    "\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"input\": \"My favorite food is pizza\"},\n",
    "    config=config\n",
    ")\n",
    "print(f\"User: My favorite food is pizza\")\n",
    "print(f\"AI: {response1}\\n\")\n",
    "\n",
    "# Later conversation (history loaded from DB)\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"input\": \"What's my favorite food?\"},\n",
    "    config=config\n",
    ")\n",
    "print(f\"User: What's my favorite food?\")\n",
    "print(f\"AI: {response2}\")\n",
    "print(\"\\n‚úÖ Conversation persisted to SQLite database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 9: Custom Memory - Token Counting\n",
    "\n",
    "Track token usage and trim when needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import tiktoken\n",
    "\n",
    "class TokenLimitedMemory:\n",
    "    \"\"\"Memory that enforces token limit.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_tokens: int = 1000):\n",
    "        self.messages = []\n",
    "        self.max_tokens = max_tokens\n",
    "        self.encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    def count_tokens(self, messages):\n",
    "        \"\"\"Count tokens in messages.\"\"\"\n",
    "        total = 0\n",
    "        for msg in messages:\n",
    "            total += len(self.encoding.encode(msg.content))\n",
    "        return total\n",
    "    \n",
    "    def add_message(self, role: str, content: str):\n",
    "        \"\"\"Add message and trim if needed.\"\"\"\n",
    "        if role == \"human\":\n",
    "            self.messages.append(HumanMessage(content=content))\n",
    "        else:\n",
    "            self.messages.append(AIMessage(content=content))\n",
    "        \n",
    "        # Trim old messages if over limit\n",
    "        while self.count_tokens(self.messages) > self.max_tokens and len(self.messages) > 1:\n",
    "            self.messages.pop(0)\n",
    "    \n",
    "    def get_messages(self):\n",
    "        return self.messages\n",
    "\n",
    "# Use custom memory\n",
    "memory = TokenLimitedMemory(max_tokens=200)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4\") | StrOutputParser()\n",
    "\n",
    "def chat(user_input: str) -> str:\n",
    "    history = memory.get_messages()\n",
    "    response = chain.invoke({\"history\": history, \"input\": user_input})\n",
    "    memory.add_message(\"human\", user_input)\n",
    "    memory.add_message(\"ai\", response)\n",
    "    return response\n",
    "\n",
    "# Test\n",
    "print(f\"Max tokens: {memory.max_tokens}\\n\")\n",
    "\n",
    "response = chat(\"Tell me a short story about a robot\")\n",
    "print(f\"Tokens after message 1: {memory.count_tokens(memory.get_messages())}\")\n",
    "\n",
    "response = chat(\"Make it longer\")\n",
    "print(f\"Tokens after message 2: {memory.count_tokens(memory.get_messages())}\")\n",
    "\n",
    "print(f\"\\nMessages in memory: {len(memory.get_messages())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 10: Memory with Agents\n",
    "\n",
    "Add memory to agents for context-aware tool use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Create tool\n",
    "@tool\n",
    "def get_user_preference(preference_type: str) -> str:\n",
    "    \"\"\"Get user's preference (mock data).\"\"\"\n",
    "    prefs = {\n",
    "        \"color\": \"blue\",\n",
    "        \"food\": \"pizza\",\n",
    "        \"language\": \"Python\"\n",
    "    }\n",
    "    return prefs.get(preference_type.lower(), \"Unknown\")\n",
    "\n",
    "tools = [get_user_preference]\n",
    "\n",
    "# Create agent\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "# Add memory\n",
    "message_history = InMemoryChatMessageHistory()\n",
    "\n",
    "agent_with_memory = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"test\"}}\n",
    "\n",
    "# Conversation\n",
    "print(\"User: What's my favorite color?\")\n",
    "result = agent_with_memory.invoke({\"input\": \"What's my favorite color?\"}, config=config)\n",
    "print(f\"AI: {result['output']}\\n\")\n",
    "\n",
    "print(\"User: What about food?\")\n",
    "result = agent_with_memory.invoke({\"input\": \"What about food?\"}, config=config)\n",
    "print(f\"AI: {result['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Memory Type Comparison\n",
    "\n",
    "### ConversationBufferMemory\n",
    "- **Stores**: All messages\n",
    "- **Pros**: Complete context, simple\n",
    "- **Cons**: Grows unbounded, expensive\n",
    "- **Use for**: Short conversations\n",
    "\n",
    "### ConversationBufferWindowMemory\n",
    "- **Stores**: Last N messages\n",
    "- **Pros**: Fixed size, predictable cost\n",
    "- **Cons**: Loses old context\n",
    "- **Use for**: Long conversations with recent context\n",
    "\n",
    "### ConversationSummaryMemory\n",
    "- **Stores**: Summary of all messages\n",
    "- **Pros**: Constant size, retains key info\n",
    "- **Cons**: Loses details, extra LLM calls\n",
    "- **Use for**: Very long conversations\n",
    "\n",
    "### ConversationSummaryBufferMemory\n",
    "- **Stores**: Recent messages + summary of old\n",
    "- **Pros**: Best of both worlds\n",
    "- **Cons**: More complex\n",
    "- **Use for**: Production chatbots\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### ‚úÖ Do\n",
    "\n",
    "1. **Use RunnableWithMessageHistory** (modern approach)\n",
    "2. **Set token limits** (prevent context overflow)\n",
    "3. **Use session IDs** (separate user conversations)\n",
    "4. **Persist to database** for production (SQLite, Redis, Postgres)\n",
    "5. **Choose appropriate memory type** based on use case\n",
    "6. **Monitor memory size** (track tokens/messages)\n",
    "7. **Implement cleanup** (delete old sessions)\n",
    "\n",
    "### ‚ùå Don't\n",
    "\n",
    "1. **Don't use unbounded memory** (will hit context limits)\n",
    "2. **Don't share sessions** across users (privacy issue)\n",
    "3. **Don't store sensitive data** without encryption\n",
    "4. **Don't ignore token costs** (memory increases costs)\n",
    "5. **Don't forget to clear memory** when appropriate\n",
    "6. **Don't use memory for stateless APIs** (unnecessary overhead)\n",
    "\n",
    "---\n",
    "\n",
    "## Production Memory Patterns\n",
    "\n",
    "### Pattern 1: Redis for Distributed Systems\n",
    "\n",
    "```python\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    return RedisChatMessageHistory(\n",
    "        session_id=session_id,\n",
    "        url=\"redis://localhost:6379\",\n",
    "        ttl=3600  # Auto-expire after 1 hour\n",
    "    )\n",
    "```\n",
    "\n",
    "### Pattern 2: PostgreSQL for Analytics\n",
    "\n",
    "```python\n",
    "from langchain_community.chat_message_histories import PostgresChatMessageHistory\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    return PostgresChatMessageHistory(\n",
    "        session_id=session_id,\n",
    "        connection_string=\"postgresql://user:pass@localhost/dbname\",\n",
    "        table_name=\"chat_history\"\n",
    "    )\n",
    "```\n",
    "\n",
    "### Pattern 3: Hybrid Memory Strategy\n",
    "\n",
    "```python\n",
    "# Recent messages in Redis (fast)\n",
    "# Older messages in Postgres (cheap)\n",
    "# Summaries for very old conversations\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Build a chatbot with memory that tracks user preferences\n",
    "# Store: name, favorite_color, favorite_food\n",
    "# Answer questions about stored preferences\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Your code here:\n",
    "# Create a chatbot that remembers user preferences\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Implement a custom memory that stores only Q&A pairs\n",
    "# Ignore conversational fluff, keep only questions and answers\n",
    "\n",
    "# Your code here:\n",
    "# Create custom memory class\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Build a multi-user chatbot with SQLite persistence\n",
    "# Each user has separate conversation history\n",
    "# History persists across sessions\n",
    "\n",
    "# Your code here:\n",
    "# Use SQLChatMessageHistory\n",
    "# Test with multiple users\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### ‚úÖ What We Learned\n",
    "\n",
    "1. **Memory Types**: Buffer, Window, Summary, SummaryBuffer\n",
    "2. **Modern Approach**: RunnableWithMessageHistory (recommended)\n",
    "3. **Session Management**: Separate conversations per user\n",
    "4. **Persistence**: SQLite, Redis, PostgreSQL\n",
    "5. **Token Management**: Monitor and limit memory size\n",
    "6. **Custom Memory**: Build domain-specific memory logic\n",
    "7. **Agent Memory**: Add context to tool-using agents\n",
    "8. **Production Patterns**: Distributed, persistent, hybrid strategies\n",
    "\n",
    "### üìä Memory Selection Guide\n",
    "\n",
    "| Use Case | Memory Type | Storage |\n",
    "|----------|-------------|--------|\n",
    "| Short chat (<10 messages) | ConversationBuffer | In-memory |\n",
    "| Medium chat (10-50 messages) | ConversationWindow | In-memory/Redis |\n",
    "| Long chat (50+ messages) | ConversationSummaryBuffer | Redis/Postgres |\n",
    "| Production chatbot | ConversationSummaryBuffer | Redis + Postgres |\n",
    "| Analytics required | ConversationBuffer | Postgres |\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "- Combine memory with RAG for document-aware conversations\n",
    "- Implement memory pruning and cleanup strategies\n",
    "- Build multi-agent systems with shared memory\n",
    "- Monitor memory performance and costs\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Memory Documentation](https://python.langchain.com/docs/modules/memory/)\n",
    "- [Chat Message Histories](https://python.langchain.com/docs/integrations/memory/)\n",
    "- [RunnableWithMessageHistory](https://python.langchain.com/docs/expression_language/how_to/message_history)\n",
    "- [Memory Types](https://python.langchain.com/docs/modules/memory/types/)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed the LangChain tutorial series. You now know:\n",
    "- ‚úÖ LangChain basics and models\n",
    "- ‚úÖ Prompt engineering\n",
    "- ‚úÖ LCEL for chain composition\n",
    "- ‚úÖ RAG for document Q&A\n",
    "- ‚úÖ Agents for tool-using systems\n",
    "- ‚úÖ Memory for conversational AI\n",
    "\n",
    "**Next**: Build production LLM applications! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
