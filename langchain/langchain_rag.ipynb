{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain RAG: Complete Retrieval-Augmented Generation Pipeline\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** combines retrieval of relevant documents with LLM generation to answer questions based on your own data.\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "RAG enables LLMs to:\n",
    "- **Answer questions** about your private documents\n",
    "- **Stay up-to-date** without retraining\n",
    "- **Reduce hallucinations** by grounding in source material\n",
    "- **Cite sources** for transparency\n",
    "- **Scale to millions** of documents\n",
    "\n",
    "### RAG Pipeline Components\n",
    "\n",
    "1. **Document Loading**: Read files (PDF, text, web, etc.)\n",
    "2. **Text Splitting**: Break into chunks\n",
    "3. **Embeddings**: Convert text to vectors\n",
    "4. **Vector Store**: Store and index embeddings\n",
    "5. **Retrieval**: Find relevant chunks\n",
    "6. **Generation**: LLM answers using retrieved context\n",
    "\n",
    "### When to Use RAG?\n",
    "\n",
    "| ‚úÖ Use RAG For | ‚ùå Don't Use For |\n",
    "|----------------|------------------|\n",
    "| Private documents | Public knowledge (use base LLM) |\n",
    "| Frequently updated data | Static, well-known facts |\n",
    "| Domain-specific Q&A | General conversation |\n",
    "| Citation needed | Creative writing |\n",
    "\n",
    "---\n",
    "\n",
    "## Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install langchain langchain-openai langchain-community\n",
    "# !pip install chromadb faiss-cpu pypdf\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API Key: \")\n",
    "\n",
    "print(\"API key configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 1: Simple RAG from Text\n",
    "\n",
    "Build a complete RAG pipeline from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Python is a high-level programming language known for its readability and simplicity.\",\n",
    "    \"Python was created by Guido van Rossum and first released in 1991.\",\n",
    "    \"Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.\",\n",
    "    \"The Zen of Python is a collection of 19 guiding principles for writing computer programs in Python.\",\n",
    "    \"Python's standard library is extensive and includes modules for everything from web development to data science.\"\n",
    "]\n",
    "\n",
    "# Step 1: Create text splitter (not needed here since docs are small)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "splits = text_splitter.create_documents(documents)\n",
    "\n",
    "# Step 2: Create embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# Step 3: Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Step 4: Create RAG prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Step 5: Create RAG chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model=\"gpt-4\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Step 6: Ask questions\n",
    "question = \"Who created Python?\"\n",
    "answer = rag_chain.invoke(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "1. **Split documents** into chunks\n",
    "2. **Embed chunks** and store in vector database\n",
    "3. **Retrieve** top 2 most relevant chunks for question\n",
    "4. **Generate** answer using retrieved context\n",
    "\n",
    "---\n",
    "\n",
    "## Example 2: Document Loaders\n",
    "\n",
    "Load documents from various sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, WebBaseLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# Text file loader\n",
    "# loader = TextLoader(\"path/to/file.txt\")\n",
    "# docs = loader.load()\n",
    "\n",
    "# PDF loader\n",
    "# loader = PyPDFLoader(\"path/to/file.pdf\")\n",
    "# docs = loader.load()\n",
    "\n",
    "# Web page loader\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/get_started/introduction\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "print(f\"First doc preview: {docs[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Loaders\n",
    "\n",
    "```python\n",
    "# Text files\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"file.txt\")\n",
    "\n",
    "# PDFs\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"file.pdf\")\n",
    "\n",
    "# Web pages\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://example.com\")\n",
    "\n",
    "# Directory (all files)\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\"./docs\", glob=\"**/*.txt\")\n",
    "\n",
    "# CSV\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "loader = CSVLoader(\"data.csv\")\n",
    "\n",
    "# JSON\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "loader = JSONLoader(\"data.json\", jq_schema=\".\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Example 3: Text Splitting Strategies\n",
    "\n",
    "Different splitters for different use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")\n",
    "\n",
    "sample_text = \"\"\"\n",
    "Python is a high-level programming language. It was created by Guido van Rossum.\n",
    "\n",
    "Python supports multiple paradigms. It is used for web development, data science, and more.\n",
    "\n",
    "The language has a large standard library. This makes it very versatile.\n",
    "\"\"\"\n",
    "\n",
    "# RecursiveCharacterTextSplitter (RECOMMENDED)\n",
    "# Tries to split on paragraphs, then sentences, then words\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "recursive_chunks = recursive_splitter.create_documents([sample_text])\n",
    "print(\"RecursiveCharacterTextSplitter:\")\n",
    "for i, chunk in enumerate(recursive_chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk.page_content}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# CharacterTextSplitter\n",
    "# Simple split by character count\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    separator=\"\\n\"\n",
    ")\n",
    "char_chunks = char_splitter.create_documents([sample_text])\n",
    "print(\"CharacterTextSplitter:\")\n",
    "for i, chunk in enumerate(char_chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk.page_content}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# TokenTextSplitter\n",
    "# Split by token count (important for staying within model limits)\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "token_chunks = token_splitter.create_documents([sample_text])\n",
    "print(\"TokenTextSplitter:\")\n",
    "for i, chunk in enumerate(token_chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Best Practices\n",
    "\n",
    "| Document Type | Chunk Size | Overlap | Splitter |\n",
    "|---------------|------------|---------|----------|\n",
    "| General text | 500-1000 | 50-100 | RecursiveCharacter |\n",
    "| Code | 300-500 | 50 | Language-specific |\n",
    "| Markdown | 500-1000 | 50-100 | MarkdownHeader |\n",
    "| Conversations | 500-1000 | 0 | RecursiveCharacter |\n",
    "\n",
    "---\n",
    "\n",
    "## Example 4: Embeddings Comparison\n",
    "\n",
    "Different embedding models have different characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "text = \"Python is a programming language\"\n",
    "\n",
    "# OpenAI embeddings (best quality, paid)\n",
    "openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "openai_vector = openai_embeddings.embed_query(text)\n",
    "print(f\"OpenAI embedding dimension: {len(openai_vector)}\")\n",
    "print(f\"First 5 values: {openai_vector[:5]}\")\n",
    "\n",
    "# HuggingFace embeddings (free, local)\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "hf_vector = hf_embeddings.embed_query(text)\n",
    "print(f\"\\nHuggingFace embedding dimension: {len(hf_vector)}\")\n",
    "print(f\"First 5 values: {hf_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Models Comparison\n",
    "\n",
    "| Model | Dimensions | Cost | Quality | Use Case |\n",
    "|-------|------------|------|---------|----------|\n",
    "| OpenAI text-embedding-3-small | 1536 | $$ | Excellent | Production |\n",
    "| OpenAI text-embedding-3-large | 3072 | $$$ | Best | High accuracy |\n",
    "| HuggingFace all-MiniLM-L6-v2 | 384 | Free | Good | Development/Local |\n",
    "| Cohere embed-english-v3.0 | 1024 | $$ | Excellent | Multilingual |\n",
    "\n",
    "---\n",
    "\n",
    "## Example 5: Vector Stores - Chroma\n",
    "\n",
    "In-memory vector store (great for development):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    \"The factory pattern is a creational design pattern.\",\n",
    "    \"The observer pattern is a behavioral design pattern.\",\n",
    "    \"The decorator pattern is a structural design pattern.\",\n",
    "    \"Creational patterns deal with object creation.\",\n",
    "    \"Behavioral patterns deal with object communication.\",\n",
    "    \"Structural patterns deal with object composition.\"\n",
    "]\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create vector store\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100)\n",
    "splits = text_splitter.create_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# Similarity search\n",
    "query = \"What is the factory pattern?\"\n",
    "results = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"Result {i}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 6: Vector Stores - FAISS\n",
    "\n",
    "High-performance vector store (production-ready):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Same documents\n",
    "docs = [\n",
    "    \"Python was created by Guido van Rossum.\",\n",
    "    \"Python is known for its simplicity.\",\n",
    "    \"Python has a large ecosystem of libraries.\",\n",
    "]\n",
    "\n",
    "# Create FAISS vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100)\n",
    "splits = text_splitter.create_documents(docs)\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# Save to disk\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "# Load from disk\n",
    "loaded_vectorstore = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Search\n",
    "results = loaded_vectorstore.similarity_search(\"Who made Python?\", k=1)\n",
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store Comparison\n",
    "\n",
    "| Vector Store | Persistence | Performance | Best For |\n",
    "|--------------|-------------|-------------|----------|\n",
    "| Chroma | Optional | Good | Development |\n",
    "| FAISS | File-based | Excellent | Local production |\n",
    "| Pinecone | Cloud | Excellent | Cloud production |\n",
    "| Weaviate | Self-hosted/Cloud | Excellent | Enterprise |\n",
    "| Qdrant | Self-hosted/Cloud | Excellent | High scale |\n",
    "\n",
    "---\n",
    "\n",
    "## Example 7: Retrieval Strategies\n",
    "\n",
    "Different ways to retrieve documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create vector store\n",
    "docs = [\n",
    "    \"Python is great for data science.\",\n",
    "    \"JavaScript is used for web development.\",\n",
    "    \"Python has many data science libraries like NumPy and Pandas.\",\n",
    "    \"JavaScript frameworks include React and Vue.\",\n",
    "    \"Data science involves statistics and machine learning.\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100)\n",
    "splits = text_splitter.create_documents(docs)\n",
    "vectorstore = Chroma.from_documents(splits, OpenAIEmbeddings())\n",
    "\n",
    "# Strategy 1: Similarity search (default)\n",
    "retriever_similarity = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2}\n",
    ")\n",
    "results = retriever_similarity.get_relevant_documents(\"Python data science\")\n",
    "print(\"Similarity search:\")\n",
    "for doc in results:\n",
    "    print(f\"- {doc.page_content}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Strategy 2: MMR (Maximum Marginal Relevance) - diverse results\n",
    "retriever_mmr = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 2, \"fetch_k\": 4}\n",
    ")\n",
    "results = retriever_mmr.get_relevant_documents(\"Python data science\")\n",
    "print(\"MMR (diverse results):\")\n",
    "for doc in results:\n",
    "    print(f\"- {doc.page_content}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Strategy 3: Similarity with threshold\n",
    "retriever_threshold = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.8}\n",
    ")\n",
    "results = retriever_threshold.get_relevant_documents(\"Python data science\")\n",
    "print(\"Similarity with threshold (score > 0.8):\")\n",
    "for doc in results:\n",
    "    print(f\"- {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 8: create_retrieval_chain (Simplified RAG)\n",
    "\n",
    "Use built-in helper for complete RAG chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create vector store\n",
    "docs = [\n",
    "    \"LangChain is a framework for developing LLM applications.\",\n",
    "    \"LangChain supports multiple LLM providers like OpenAI and Anthropic.\",\n",
    "    \"RAG is a technique to augment LLMs with external knowledge.\",\n",
    "    \"LCEL is LangChain's expression language for building chains.\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100)\n",
    "splits = text_splitter.create_documents(docs)\n",
    "vectorstore = Chroma.from_documents(splits, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Create prompt\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create chains\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# Ask question\n",
    "result = rag_chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "print(f\"Question: {result['input']}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"\\nSource documents:\")\n",
    "for i, doc in enumerate(result['context'], 1):\n",
    "    print(f\"{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 9: Custom RAG Chain with LCEL\n",
    "\n",
    "Build RAG from scratch with full control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Create vector store\n",
    "docs = [\n",
    "    \"Design patterns are reusable solutions to common software problems.\",\n",
    "    \"The singleton pattern ensures a class has only one instance.\",\n",
    "    \"The factory pattern provides an interface for creating objects.\",\n",
    "    \"The observer pattern defines one-to-many dependencies between objects.\"\n",
    "]\n",
    "vectorstore = Chroma.from_texts(docs, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Custom prompt\n",
    "template = \"\"\"You are a helpful assistant. Answer the question based on the context below.\n",
    "If you can't answer, say \"I don't have enough information.\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Helper function\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build custom RAG chain with LCEL\n",
    "rag_chain = (\n",
    "    RunnableParallel(\n",
    "        context=retriever | format_docs,\n",
    "        question=RunnablePassthrough()\n",
    "    )\n",
    "    | prompt\n",
    "    | ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test it\n",
    "answer = rag_chain.invoke(\"What is the factory pattern?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 10: RAG with Chat History\n",
    "\n",
    "Contextualized RAG that understands follow-up questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Create vector store\n",
    "docs = [\n",
    "    \"Python was created by Guido van Rossum in 1991.\",\n",
    "    \"Python is known for its simple and readable syntax.\",\n",
    "    \"Python has a large ecosystem including Django, Flask, and NumPy.\"\n",
    "]\n",
    "vectorstore = Chroma.from_texts(docs, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Contextualize question (rewrite based on chat history)\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", contextualize_q_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "# Answer question\n",
    "qa_system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, say that you don't know.\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", qa_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Complete RAG chain\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "# Conversation\n",
    "chat_history = []\n",
    "\n",
    "# First question\n",
    "result = rag_chain.invoke({\n",
    "    \"input\": \"Who created Python?\",\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "print(f\"Q: Who created Python?\")\n",
    "print(f\"A: {result['answer']}\\n\")\n",
    "\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=\"Who created Python?\"),\n",
    "    AIMessage(content=result[\"answer\"])\n",
    "])\n",
    "\n",
    "# Follow-up question (uses chat history!)\n",
    "result = rag_chain.invoke({\n",
    "    \"input\": \"What year?\",  # Refers to creation year from previous question\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "print(f\"Q: What year?\")\n",
    "print(f\"A: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Advanced Pattern: Multi-Query RAG\n",
    "\n",
    "Generate multiple queries for better retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Create vector store\n",
    "docs = [\n",
    "    \"Machine learning is a subset of AI focused on learning from data.\",\n",
    "    \"Deep learning uses neural networks with multiple layers.\",\n",
    "    \"Supervised learning uses labeled data for training.\",\n",
    "    \"Unsupervised learning finds patterns without labels.\"\n",
    "]\n",
    "vectorstore = Chroma.from_texts(docs, OpenAIEmbeddings())\n",
    "base_retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Multi-query retriever (generates alternative questions)\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=base_retriever,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Single query retrieves using multiple generated questions\n",
    "results = retriever.get_relevant_documents(\"What is ML?\")\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Advanced Pattern: Parent Document Retriever\n",
    "\n",
    "Retrieve small chunks but return larger parent documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Sample document\n",
    "full_doc = \"\"\"\n",
    "Python Programming Language.\n",
    "\n",
    "Python is a high-level, interpreted programming language. It was created by Guido van Rossum and first released in 1991.\n",
    "\n",
    "Python emphasizes code readability with significant whitespace. It supports multiple programming paradigms including procedural, object-oriented, and functional programming.\n",
    "\n",
    "Python has a large standard library and ecosystem. Popular frameworks include Django for web development and NumPy for numerical computing.\n",
    "\"\"\"\n",
    "\n",
    "# Parent splitter (large chunks)\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "# Child splitter (small chunks for retrieval)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=50)\n",
    "\n",
    "# Storage for parent documents\n",
    "store = InMemoryStore()\n",
    "vectorstore = Chroma(embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# Create retriever\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "retriever.add_documents([{\"page_content\": full_doc}])\n",
    "\n",
    "# Retrieves based on small chunks but returns large parent\n",
    "results = retriever.get_relevant_documents(\"Who created Python?\")\n",
    "print(\"Retrieved parent document:\")\n",
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### ‚úÖ Do\n",
    "\n",
    "1. **Choose appropriate chunk size** (500-1000 for general text)\n",
    "2. **Add chunk overlap** (10-20% of chunk size)\n",
    "3. **Use RecursiveCharacterTextSplitter** (respects document structure)\n",
    "4. **Include metadata** (source, page number, etc.)\n",
    "5. **Test retrieval quality** (check if right docs are retrieved)\n",
    "6. **Use MMR for diversity** (avoid redundant results)\n",
    "7. **Add citations** (return source documents)\n",
    "\n",
    "### ‚ùå Don't\n",
    "\n",
    "1. **Don't use tiny chunks** (<100 chars loses context)\n",
    "2. **Don't use huge chunks** (>2000 chars too much noise)\n",
    "3. **Don't skip overlap** (loses continuity between chunks)\n",
    "4. **Don't ignore document structure** (split mid-sentence)\n",
    "5. **Don't retrieve too many docs** (context window limit)\n",
    "6. **Don't ignore retrieval metrics** (precision/recall)\n",
    "\n",
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "### ‚ùå Mistake 1: Chunks Too Large\n",
    "\n",
    "```python\n",
    "# Bad - chunks too large, too much irrelevant info\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=5000)\n",
    "```\n",
    "\n",
    "**Solution**: Use 500-1000 character chunks.\n",
    "\n",
    "### ‚ùå Mistake 2: No Overlap\n",
    "\n",
    "```python\n",
    "# Bad - loses context at chunk boundaries\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "```\n",
    "\n",
    "**Solution**: Add 10-20% overlap.\n",
    "\n",
    "### ‚ùå Mistake 3: Not Testing Retrieval\n",
    "\n",
    "```python\n",
    "# Bad - assumes retrieval works without testing\n",
    "retriever = vectorstore.as_retriever()\n",
    "```\n",
    "\n",
    "**Solution**: Test with sample queries:\n",
    "```python\n",
    "# Test retrieval quality\n",
    "test_query = \"your test question\"\n",
    "docs = retriever.get_relevant_documents(test_query)\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Build a RAG system for code documentation\n",
    "# Load Python docstrings, split appropriately, enable Q&A\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Sample code documentation\n",
    "code_docs = [\n",
    "    \"\"\"def calculate_sum(numbers: List[int]) -> int:\n",
    "    '''Calculate the sum of a list of numbers.\n",
    "    \n",
    "    Args:\n",
    "        numbers: List of integers to sum\n",
    "    \n",
    "    Returns:\n",
    "        The sum of all numbers\n",
    "    '''\"\"\",\n",
    "    \"\"\"def filter_even(numbers: List[int]) -> List[int]:\n",
    "    '''Filter even numbers from a list.\n",
    "    \n",
    "    Args:\n",
    "        numbers: List of integers\n",
    "    \n",
    "    Returns:\n",
    "        List containing only even numbers\n",
    "    '''\"\"\"\n",
    "]\n",
    "\n",
    "# Your code here: Build RAG system for code Q&A\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Implement RAG with source citations\n",
    "# Answer questions and include which documents were used\n",
    "\n",
    "# Your code here:\n",
    "# Build a chain that returns both answer and source documents\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Build RAG with different retrieval strategies\n",
    "# Compare similarity search vs MMR for the same query\n",
    "\n",
    "# Your code here:\n",
    "# Create two retrievers and compare results\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### ‚úÖ What We Learned\n",
    "\n",
    "1. **RAG Pipeline**: Load ‚Üí Split ‚Üí Embed ‚Üí Store ‚Üí Retrieve ‚Üí Generate\n",
    "2. **Document Loaders**: TextLoader, PyPDFLoader, WebBaseLoader, etc.\n",
    "3. **Text Splitting**: RecursiveCharacterTextSplitter (recommended)\n",
    "4. **Embeddings**: OpenAI (best), HuggingFace (free)\n",
    "5. **Vector Stores**: Chroma (dev), FAISS (prod), Pinecone (cloud)\n",
    "6. **Retrieval Strategies**: Similarity, MMR, threshold\n",
    "7. **create_retrieval_chain**: Built-in RAG helper\n",
    "8. **Chat History**: Contextualize questions with history\n",
    "9. **Advanced Patterns**: Multi-query, parent document retrieval\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "- **langchain_agents.ipynb**: Combine RAG with agents\n",
    "- **langchain_memory.ipynb**: Advanced conversation memory\n",
    "- Production RAG: Evaluation, monitoring, optimization\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/)\n",
    "- [Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)\n",
    "- [Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)\n",
    "- [Vector Stores](https://python.langchain.com/docs/integrations/vectorstores/)\n",
    "- [Retrieval Strategies](https://python.langchain.com/docs/modules/data_connection/retrievers/)\n",
    "\n",
    "---\n",
    "\n",
    "**Next Notebook**: `langchain_agents.ipynb` - Build intelligent agents with tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
