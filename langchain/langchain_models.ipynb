{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Models: LLMs, Chat Models & Providers\n",
    "\n",
    "## Introduction\n",
    "\n",
    "LangChain supports multiple model types and providers, allowing you to write provider-agnostic code that can switch between OpenAI, Anthropic, Google, and others with minimal changes.\n",
    "\n",
    "### Model Types\n",
    "\n",
    "1. **Chat Models** (Recommended): Message-based interface (System, Human, AI messages)\n",
    "2. **LLMs** (Legacy): Simple text in, text out (being phased out)\n",
    "3. **Embeddings**: Convert text to vector representations\n",
    "\n",
    "---\n",
    "\n",
    "## Chat Models vs LLMs\n",
    "\n",
    "### Chat Models (Modern Standard)\n",
    "\n",
    "- Input: List of messages (SystemMessage, HumanMessage, AIMessage)\n",
    "- Output: AIMessage object\n",
    "- Use for: GPT-4, Claude, Gemini, and all modern conversational models\n",
    "\n",
    "### LLMs (Legacy)\n",
    "\n",
    "- Input: String\n",
    "- Output: String\n",
    "- Use for: Older completion models (GPT-3, etc.)\n",
    "\n",
    "**Recommendation**: Always use Chat Models for new projects.\n",
    "\n",
    "---\n",
    "\n",
    "## Supported Providers (2025)\n",
    "\n",
    "| Provider | Package | Models | Best For |\n",
    "|----------|---------|--------|----------|\n",
    "| OpenAI | `langchain-openai` | GPT-4, GPT-3.5 | General purpose, function calling |\n",
    "| Anthropic | `langchain-anthropic` | Claude 3/4 | Long context, reasoning, safety |\n",
    "| Google | `langchain-google-genai` | Gemini Pro | Multimodal, free tier |\n",
    "| Cohere | `langchain-cohere` | Command R+ | RAG, search |\n",
    "| Mistral | `langchain-mistral` | Mistral Large | Open source alternative |\n",
    "| Local | `langchain-ollama` | Llama, Mistral | Privacy, offline |\n",
    "\n",
    "---\n",
    "\n",
    "## Setup: Multiple Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set API keys for different providers\n",
    "# You can choose one or multiple providers\n",
    "\n",
    "# OpenAI\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API Key (or press Enter to skip): \") or \"skip\"\n",
    "\n",
    "# Anthropic\n",
    "if not os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = getpass(\"Enter Anthropic API Key (or press Enter to skip): \") or \"skip\"\n",
    "\n",
    "# Google\n",
    "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Enter Google API Key (or press Enter to skip): \") or \"skip\"\n",
    "\n",
    "print(\"\\nAPI keys configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 1: OpenAI Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize OpenAI model\n",
    "llm_openai = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# Create messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful Python programming assistant.\"),\n",
    "    HumanMessage(content=\"Explain list comprehensions in one sentence.\")\n",
    "]\n",
    "\n",
    "# Invoke the model\n",
    "response = llm_openai.invoke(messages)\n",
    "print(f\"OpenAI Response:\\n{response.content}\")\n",
    "print(f\"\\nMetadata: {response.response_metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 2: Anthropic Claude Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Initialize Anthropic model\n",
    "llm_claude = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# Same messages as before!\n",
    "response = llm_claude.invoke(messages)\n",
    "print(f\"Claude Response:\\n{response.content}\")\n",
    "print(f\"\\nMetadata: {response.response_metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 3: Google Gemini Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize Google Gemini model\n",
    "llm_gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-pro\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# Same messages as before!\n",
    "response = llm_gemini.invoke(messages)\n",
    "print(f\"Gemini Response:\\n{response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight: Provider Independence\n",
    "\n",
    "Notice how all three providers:\n",
    "- Accept the same message format\n",
    "- Use the same `.invoke()` method\n",
    "- Return AIMessage objects\n",
    "\n",
    "This makes switching providers trivial!\n",
    "\n",
    "---\n",
    "\n",
    "## Model Parameters\n",
    "\n",
    "### Common Parameters (All Providers)\n",
    "\n",
    "```python\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4\",              # Model name\n",
    "    temperature=0.7,            # 0.0 (deterministic) to 2.0 (creative)\n",
    "    max_tokens=500,             # Maximum tokens in response\n",
    "    timeout=30,                 # Request timeout in seconds\n",
    "    max_retries=2,              # Number of retries on failure\n",
    ")\n",
    "```\n",
    "\n",
    "### Temperature Guide\n",
    "\n",
    "| Temperature | Use Case | Example |\n",
    "|-------------|----------|----------|\n",
    "| 0.0 - 0.3 | Deterministic, factual | Code generation, Q&A |\n",
    "| 0.4 - 0.7 | Balanced | General conversation |\n",
    "| 0.8 - 1.2 | Creative | Story writing, brainstorming |\n",
    "| 1.3 - 2.0 | Very creative | Experimental, artistic |\n",
    "\n",
    "---\n",
    "\n",
    "## Example 4: Temperature Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = \"Write a creative name for a coffee shop\"\n",
    "\n",
    "temperatures = [0.0, 0.5, 1.0, 1.5]\n",
    "\n",
    "for temp in temperatures:\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=temp)\n",
    "    response = llm.invoke(prompt)\n",
    "    print(f\"Temperature {temp}: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 5: Streaming Responses\n",
    "\n",
    "Stream tokens as they're generated for better UX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "print(\"Streaming response:\\n\")\n",
    "for chunk in llm.stream(\"Write a haiku about Python programming\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 6: Async Streaming\n",
    "\n",
    "For non-blocking streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "async def async_stream_example():\n",
    "    print(\"Async streaming:\\n\")\n",
    "    async for chunk in llm.astream(\"Explain asyncio in Python\"):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n\\nDone!\")\n",
    "\n",
    "# Run in Jupyter\n",
    "await async_stream_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 7: Multi-Model Comparison\n",
    "\n",
    "Compare responses from different providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def compare_models(prompt: str):\n",
    "    \"\"\"Compare same prompt across multiple providers.\"\"\"\n",
    "    \n",
    "    models = {\n",
    "        \"OpenAI GPT-4\": ChatOpenAI(model=\"gpt-4\", temperature=0.7),\n",
    "        \"Anthropic Claude\": ChatAnthropic(model=\"claude-sonnet-4-5-20250929\", temperature=0.7),\n",
    "        \"Google Gemini\": ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)\n",
    "    }\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            response = model.invoke(prompt)\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(response.content)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n{name}: Error - {e}\")\n",
    "\n",
    "# Try it\n",
    "compare_models(\"What are the key benefits of using design patterns in software development?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 8: Token Usage Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Track token usage and cost\n",
    "with get_openai_callback() as cb:\n",
    "    response1 = llm.invoke(\"What is Python?\")\n",
    "    response2 = llm.invoke(\"What is JavaScript?\")\n",
    "    \n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 9: Model Fallback\n",
    "\n",
    "Try multiple models with automatic fallback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.runnables import RunnableWithFallbacks\n",
    "\n",
    "# Primary model\n",
    "primary = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "\n",
    "# Fallback models\n",
    "fallback1 = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n",
    "fallback2 = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Create fallback chain\n",
    "model_with_fallbacks = primary.with_fallbacks([fallback1, fallback2])\n",
    "\n",
    "# If GPT-4 fails, tries Claude, then GPT-3.5\n",
    "response = model_with_fallbacks.invoke(\"Explain Python decorators\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 10: Caching Responses\n",
    "\n",
    "Cache LLM responses to save costs and improve performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "import time\n",
    "\n",
    "# Enable caching\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "# First call - hits the API\n",
    "start = time.time()\n",
    "response1 = llm.invoke(prompt)\n",
    "time1 = time.time() - start\n",
    "print(f\"First call: {time1:.2f}s\")\n",
    "print(response1.content)\n",
    "\n",
    "# Second call - uses cache\n",
    "start = time.time()\n",
    "response2 = llm.invoke(prompt)\n",
    "time2 = time.time() - start\n",
    "print(f\"\\nSecond call (cached): {time2:.2f}s\")\n",
    "print(response2.content)\n",
    "\n",
    "print(f\"\\nSpeedup: {time1/time2:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache Types\n",
    "\n",
    "```python\n",
    "# In-memory (development)\n",
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# SQLite (persistent)\n",
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n",
    "\n",
    "# Redis (production)\n",
    "from langchain.cache import RedisCache\n",
    "set_llm_cache(RedisCache(redis_url=\"redis://localhost:6379\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Example 11: Embeddings\n",
    "\n",
    "Convert text to vector representations (for RAG, similarity search):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_anthropic import AnthropicEmbeddings\n",
    "\n",
    "# OpenAI embeddings\n",
    "embeddings_openai = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Embed a single text\n",
    "text = \"LangChain makes building LLM applications easy\"\n",
    "vector = embeddings_openai.embed_query(text)\n",
    "\n",
    "print(f\"Vector dimension: {len(vector)}\")\n",
    "print(f\"First 5 values: {vector[:5]}\")\n",
    "\n",
    "# Embed multiple texts\n",
    "texts = [\"Python is great\", \"JavaScript is popular\", \"Rust is fast\"]\n",
    "vectors = embeddings_openai.embed_documents(texts)\n",
    "print(f\"\\nEmbedded {len(vectors)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Models Comparison\n",
    "\n",
    "| Provider | Model | Dimensions | Cost | Best For |\n",
    "|----------|-------|------------|------|----------|\n",
    "| OpenAI | text-embedding-3-small | 1536 | Low | General purpose |\n",
    "| OpenAI | text-embedding-3-large | 3072 | Medium | High accuracy |\n",
    "| Cohere | embed-english-v3.0 | 1024 | Medium | Search, RAG |\n",
    "| HuggingFace | all-MiniLM-L6-v2 | 384 | Free | Local, offline |\n",
    "\n",
    "---\n",
    "\n",
    "## Example 12: Function Calling (OpenAI)\n",
    "\n",
    "Let the model call functions with structured outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Define function schema using Pydantic\n",
    "class GetWeather(BaseModel):\n",
    "    \"\"\"Get the current weather for a location.\"\"\"\n",
    "    location: str = Field(description=\"The city and state, e.g., San Francisco, CA\")\n",
    "    unit: str = Field(default=\"celsius\", description=\"Temperature unit\")\n",
    "\n",
    "# Bind function to model\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "llm_with_tools = llm.bind_tools([GetWeather])\n",
    "\n",
    "# Model will decide to call the function\n",
    "response = llm_with_tools.invoke(\"What's the weather in Paris?\")\n",
    "print(response)\n",
    "\n",
    "# Check if model wants to call a function\n",
    "if response.tool_calls:\n",
    "    tool_call = response.tool_calls[0]\n",
    "    print(f\"\\nFunction: {tool_call['name']}\")\n",
    "    print(f\"Arguments: {tool_call['args']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Provider-Specific Features\n",
    "\n",
    "### OpenAI\n",
    "\n",
    "- âœ… Function calling\n",
    "- âœ… JSON mode\n",
    "- âœ… Vision (GPT-4V)\n",
    "- âœ… Best ecosystem support\n",
    "\n",
    "### Anthropic Claude\n",
    "\n",
    "- âœ… 200K token context window\n",
    "- âœ… Superior reasoning\n",
    "- âœ… Strong safety guardrails\n",
    "- âœ… Constitutional AI\n",
    "\n",
    "### Google Gemini\n",
    "\n",
    "- âœ… Multimodal (text, images, video)\n",
    "- âœ… Free tier available\n",
    "- âœ… Fast inference\n",
    "- âœ… Integrated with Google services\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### âœ… Do\n",
    "\n",
    "1. **Use Chat Models** for all new projects (not legacy LLMs)\n",
    "2. **Set timeouts** to prevent hanging requests\n",
    "3. **Implement retries** for production reliability\n",
    "4. **Cache responses** when appropriate\n",
    "5. **Track token usage** to monitor costs\n",
    "6. **Use fallbacks** for high availability\n",
    "\n",
    "### âŒ Don't\n",
    "\n",
    "1. **Don't hardcode API keys** in code (use environment variables)\n",
    "2. **Don't ignore rate limits** (implement backoff)\n",
    "3. **Don't use high temperature** for factual tasks\n",
    "4. **Don't forget error handling** (network issues, API errors)\n",
    "5. **Don't batch without limits** (respect rate limits)\n",
    "\n",
    "---\n",
    "\n",
    "## Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Create a function that tries multiple providers\n",
    "# If one fails, automatically try the next\n",
    "\n",
    "def get_response_with_fallback(prompt: str) -> str:\n",
    "    \"\"\"Try multiple providers, return first successful response.\"\"\"\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_anthropic import ChatAnthropic\n",
    "    \n",
    "    models = [\n",
    "        ChatOpenAI(model=\"gpt-4\"),\n",
    "        ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"),\n",
    "        ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    ]\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        try:\n",
    "            response = model.invoke(prompt)\n",
    "            print(f\"Success with model {i+1}\")\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            print(f\"Model {i+1} failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return \"All models failed\"\n",
    "\n",
    "# Test it\n",
    "result = get_response_with_fallback(\"Explain Python generators\")\n",
    "print(f\"\\nResult: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Compare response times across providers\n",
    "import time\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "def benchmark_models(prompt: str):\n",
    "    \"\"\"Benchmark response time for different models.\"\"\"\n",
    "    models = {\n",
    "        \"GPT-4\": ChatOpenAI(model=\"gpt-4\"),\n",
    "        \"GPT-3.5\": ChatOpenAI(model=\"gpt-3.5-turbo\"),\n",
    "        \"Claude\": ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            start = time.time()\n",
    "            response = model.invoke(prompt)\n",
    "            elapsed = time.time() - start\n",
    "            results[name] = {\n",
    "                \"time\": elapsed,\n",
    "                \"response_length\": len(response.content)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            results[name] = {\"error\": str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test it\n",
    "results = benchmark_models(\"Explain the factory pattern in 2 sentences\")\n",
    "for model, data in results.items():\n",
    "    if \"error\" in data:\n",
    "        print(f\"{model}: Error - {data['error']}\")\n",
    "    else:\n",
    "        print(f\"{model}: {data['time']:.2f}s ({data['response_length']} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Create a cost estimator\n",
    "# Calculate approximate cost for a given prompt and number of completions\n",
    "\n",
    "def estimate_cost(prompt: str, num_completions: int, model_name: str = \"gpt-4\") -> dict:\n",
    "    \"\"\"Estimate cost for multiple completions.\"\"\"\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.callbacks import get_openai_callback\n",
    "    \n",
    "    llm = ChatOpenAI(model=model_name)\n",
    "    \n",
    "    with get_openai_callback() as cb:\n",
    "        # Make one call to estimate\n",
    "        llm.invoke(prompt)\n",
    "        \n",
    "        # Extrapolate\n",
    "        estimated_tokens = cb.total_tokens * num_completions\n",
    "        estimated_cost = cb.total_cost * num_completions\n",
    "    \n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"completions\": num_completions,\n",
    "        \"estimated_tokens\": estimated_tokens,\n",
    "        \"estimated_cost_usd\": estimated_cost\n",
    "    }\n",
    "\n",
    "# Test it\n",
    "estimate = estimate_cost(\"Explain Python asyncio\", 100, \"gpt-4\")\n",
    "print(f\"Estimated cost for {estimate['completions']} completions:\")\n",
    "print(f\"Tokens: {estimate['estimated_tokens']}\")\n",
    "print(f\"Cost: ${estimate['estimated_cost_usd']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### âœ… What We Learned\n",
    "\n",
    "1. **Provider Independence**: Same interface for OpenAI, Anthropic, Google\n",
    "2. **Chat Models**: Modern message-based interface (recommended)\n",
    "3. **Temperature Control**: 0.0 (deterministic) to 2.0 (creative)\n",
    "4. **Streaming**: Better UX with token-by-token responses\n",
    "5. **Caching**: Save costs and improve performance\n",
    "6. **Fallbacks**: High availability with multiple providers\n",
    "7. **Token Tracking**: Monitor usage and costs\n",
    "8. **Embeddings**: Convert text to vectors for RAG\n",
    "\n",
    "### ðŸ“š Next Steps\n",
    "\n",
    "- **langchain_prompts.ipynb**: Advanced prompt engineering\n",
    "- **langchain_lcel.ipynb**: Master the Expression Language\n",
    "- **langchain_rag.ipynb**: Build RAG applications\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [OpenAI Models](https://platform.openai.com/docs/models)\n",
    "- [Anthropic Claude](https://www.anthropic.com/claude)\n",
    "- [Google Gemini](https://ai.google.dev/)\n",
    "- [LangChain Model Documentation](https://python.langchain.com/docs/integrations/chat/)\n",
    "\n",
    "---\n",
    "\n",
    "**Next Notebook**: `langchain_prompts.ipynb` - Prompt engineering techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
