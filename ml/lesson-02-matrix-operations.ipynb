{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Matrix Operations for Machine Learning\n",
    "\n",
    "**Duration**: 4-5 hours  \n",
    "**Prerequisites**: Lesson 1 (Vector Fundamentals), basic linear algebra  \n",
    "**Learning Objectives**:\n",
    "- Master matrix creation and manipulation with NumPy\n",
    "- Understand matrix multiplication and its ML applications\n",
    "- Implement key matrix operations (transpose, inverse, decompositions)\n",
    "- Apply matrices to solve real ML problems\n",
    "- Build foundation for neural networks and linear models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Matrices in ML (Theory)\n",
    "\n",
    "### What are Matrices?\n",
    "\n",
    "A **matrix** is a 2D array of numbers arranged in rows and columns. In machine learning:\n",
    "- **Datasets** are stored as matrices (rows = samples, columns = features)\n",
    "- **Neural network weights** are matrices\n",
    "- **Image data** is represented as matrices (or 3D tensors)\n",
    "- **Transformations** are performed using matrix operations\n",
    "\n",
    "### Why Matrices are Crucial in ML\n",
    "\n",
    "1. **Data Representation**: Entire datasets fit into matrix format\n",
    "2. **Vectorized Operations**: Process all data simultaneously\n",
    "3. **Linear Transformations**: Core of most ML algorithms\n",
    "4. **Neural Networks**: Weight matrices define network behavior\n",
    "5. **Dimensionality Reduction**: PCA, SVD use matrix decompositions\n",
    "\n",
    "### Matrix Terminology\n",
    "\n",
    "- **Shape**: (rows, columns) - e.g., (3, 4) matrix has 3 rows and 4 columns\n",
    "- **Square Matrix**: Same number of rows and columns\n",
    "- **Identity Matrix**: Square matrix with 1s on diagonal, 0s elsewhere\n",
    "- **Transpose**: Flip rows and columns (A^T)\n",
    "- **Inverse**: Matrix A^(-1) where A × A^(-1) = I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Matrix Creation and Basic Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=== Matrix Creation Methods ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: From nested lists\n",
    "matrix_from_list = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "print(f\"Matrix from list:\")\n",
    "print(matrix_from_list)\n",
    "print(f\"Shape: {matrix_from_list.shape}\")\n",
    "print(f\"Size (total elements): {matrix_from_list.size}\")\n",
    "print(f\"Number of dimensions: {matrix_from_list.ndim}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using NumPy functions\n",
    "zeros_matrix = np.zeros((3, 4))  # 3 rows, 4 columns of zeros\n",
    "ones_matrix = np.ones((2, 3))    # 2 rows, 3 columns of ones\n",
    "identity_matrix = np.eye(3)      # 3x3 identity matrix\n",
    "random_matrix = np.random.random((2, 3))  # Random values [0, 1)\n",
    "\n",
    "print(\"Zeros matrix (3x4):\")\n",
    "print(zeros_matrix)\n",
    "print(\"\\nOnes matrix (2x3):\")\n",
    "print(ones_matrix)\n",
    "print(\"\\nIdentity matrix (3x3):\")\n",
    "print(identity_matrix)\n",
    "print(\"\\nRandom matrix (2x3):\")\n",
    "print(random_matrix)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Reshaping vectors into matrices\n",
    "vector = np.arange(1, 13)  # [1, 2, 3, ..., 12]\n",
    "matrix_3x4 = vector.reshape(3, 4)\n",
    "matrix_4x3 = vector.reshape(4, 3)\n",
    "matrix_2x6 = vector.reshape(2, 6)\n",
    "\n",
    "print(f\"Original vector: {vector}\")\n",
    "print(f\"\\nReshaped to 3x4:\")\n",
    "print(matrix_3x4)\n",
    "print(f\"\\nReshaped to 4x3:\")\n",
    "print(matrix_4x3)\n",
    "print(f\"\\nReshaped to 2x6:\")\n",
    "print(matrix_2x6)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4: Special matrices for ML\n",
    "def create_dataset_matrix(n_samples: int, n_features: int, random_seed: int = 42) -> np.ndarray:\n",
    "    \"\"\"Create a synthetic dataset matrix\"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    return np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Create sample dataset: 100 samples, 5 features\n",
    "dataset = create_dataset_matrix(100, 5)\n",
    "print(f\"Dataset matrix shape: {dataset.shape}\")\n",
    "print(f\"First 5 samples:\")\n",
    "print(dataset[:5])  # Show first 5 rows\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"Mean: {np.mean(dataset, axis=0)}\")\n",
    "print(f\"Std: {np.std(dataset, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Matrix Indexing and Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample matrix for indexing examples\n",
    "sample_matrix = np.array([\n",
    "    [10, 20, 30, 40],\n",
    "    [50, 60, 70, 80],\n",
    "    [90, 100, 110, 120]\n",
    "])\n",
    "\n",
    "print(\"Sample matrix:\")\n",
    "print(sample_matrix)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic indexing\n",
    "print(\"=== Basic Indexing ===\")\n",
    "print(f\"Element at [1, 2]: {sample_matrix[1, 2]}\")  # Row 1, Column 2\n",
    "print(f\"Element at [0, 3]: {sample_matrix[0, 3]}\")  # Row 0, Column 3\n",
    "print()\n",
    "\n",
    "# Row and column selection\n",
    "print(\"=== Row and Column Selection ===\")\n",
    "print(f\"Row 1: {sample_matrix[1, :]}\")\n",
    "print(f\"Column 2: {sample_matrix[:, 2]}\")\n",
    "print(f\"Last row: {sample_matrix[-1, :]}\")\n",
    "print(f\"Last column: {sample_matrix[:, -1]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced slicing\n",
    "print(\"=== Advanced Slicing ===\")\n",
    "print(f\"First 2 rows, first 3 columns:\")\n",
    "print(sample_matrix[:2, :3])\n",
    "print(f\"\\nRows 1-2, columns 1-3:\")\n",
    "print(sample_matrix[1:3, 1:4])\n",
    "print(f\"\\nEvery other element (step=2):\")\n",
    "print(sample_matrix[::2, ::2])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean indexing - very useful in ML!\n",
    "print(\"=== Boolean Indexing ===\")\n",
    "condition = sample_matrix > 70\n",
    "print(f\"Elements > 70:\")\n",
    "print(sample_matrix[condition])\n",
    "print(f\"\\nBoolean mask:\")\n",
    "print(condition)\n",
    "print(f\"\\nSet elements > 70 to 999:\")\n",
    "matrix_copy = sample_matrix.copy()\n",
    "matrix_copy[matrix_copy > 70] = 999\n",
    "print(matrix_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Essential Matrix Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrices for operations\n",
    "A = np.array([\n",
    "    [1, 2],\n",
    "    [3, 4]\n",
    "])\n",
    "\n",
    "B = np.array([\n",
    "    [5, 6],\n",
    "    [7, 8]\n",
    "])\n",
    "\n",
    "C = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "\n",
    "print(f\"Matrix A:\")\n",
    "print(A)\n",
    "print(f\"\\nMatrix B:\")\n",
    "print(B)\n",
    "print(f\"\\nMatrix C:\")\n",
    "print(C)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic arithmetic operations (element-wise)\n",
    "print(\"=== Element-wise Operations ===\")\n",
    "print(f\"A + B:\")\n",
    "print(A + B)\n",
    "print(f\"\\nA - B:\")\n",
    "print(A - B)\n",
    "print(f\"\\nA * B (element-wise):\")\n",
    "print(A * B)\n",
    "print(f\"\\nA / B:\")\n",
    "print(A / B)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication - THE MOST IMPORTANT OPERATION!\n",
    "print(\"=== Matrix Multiplication ===\")\n",
    "\n",
    "# Method 1: Using @ operator (recommended)\n",
    "matrix_mult_1 = A @ B\n",
    "print(f\"A @ B:\")\n",
    "print(matrix_mult_1)\n",
    "\n",
    "# Method 2: Using np.dot()\n",
    "matrix_mult_2 = np.dot(A, B)\n",
    "print(f\"\\nnp.dot(A, B):\")\n",
    "print(matrix_mult_2)\n",
    "\n",
    "# Method 3: Using np.matmul()\n",
    "matrix_mult_3 = np.matmul(A, B)\n",
    "print(f\"\\nnp.matmul(A, B):\")\n",
    "print(matrix_mult_3)\n",
    "\n",
    "print(f\"\\nAll methods give same result: {np.array_equal(matrix_mult_1, matrix_mult_2) and np.array_equal(matrix_mult_2, matrix_mult_3)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix-vector multiplication (crucial for ML)\n",
    "print(\"=== Matrix-Vector Multiplication ===\")\n",
    "vector = np.array([1, 2])\n",
    "result = A @ vector\n",
    "print(f\"Matrix A:\")\n",
    "print(A)\n",
    "print(f\"Vector: {vector}\")\n",
    "print(f\"A @ vector = {result}\")\n",
    "print(f\"Manual calculation: [{A[0,0]*vector[0] + A[0,1]*vector[1]}, {A[1,0]*vector[0] + A[1,1]*vector[1]}]\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose operation\n",
    "print(\"=== Matrix Transpose ===\")\n",
    "print(f\"Original matrix C:\")\n",
    "print(C)\n",
    "print(f\"Shape: {C.shape}\")\n",
    "print(f\"\\nTranspose C.T:\")\n",
    "print(C.T)\n",
    "print(f\"Shape: {C.T.shape}\")\n",
    "print(f\"\\nTranspose using np.transpose():\")\n",
    "print(np.transpose(C))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix properties and statistics\n",
    "print(\"=== Matrix Properties ===\")\n",
    "test_matrix = np.random.randn(4, 3)\n",
    "print(f\"Test matrix:\")\n",
    "print(test_matrix)\n",
    "print(f\"\\nShape: {test_matrix.shape}\")\n",
    "print(f\"Size: {test_matrix.size}\")\n",
    "print(f\"Mean (overall): {np.mean(test_matrix):.3f}\")\n",
    "print(f\"Mean by rows: {np.mean(test_matrix, axis=1)}\")\n",
    "print(f\"Mean by columns: {np.mean(test_matrix, axis=0)}\")\n",
    "print(f\"Standard deviation: {np.std(test_matrix):.3f}\")\n",
    "print(f\"Min value: {np.min(test_matrix):.3f}\")\n",
    "print(f\"Max value: {np.max(test_matrix):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Advanced Matrix Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix inverse (for square matrices)\n",
    "print(\"=== Matrix Inverse ===\")\n",
    "square_matrix = np.array([\n",
    "    [4, 2],\n",
    "    [3, 1]\n",
    "])\n",
    "\n",
    "print(f\"Original matrix:\")\n",
    "print(square_matrix)\n",
    "\n",
    "# Calculate inverse\n",
    "inverse_matrix = np.linalg.inv(square_matrix)\n",
    "print(f\"\\nInverse matrix:\")\n",
    "print(inverse_matrix)\n",
    "\n",
    "# Verify: A * A^(-1) = I\n",
    "identity_check = square_matrix @ inverse_matrix\n",
    "print(f\"\\nA @ A^(-1) (should be identity):\")\n",
    "print(identity_check)\n",
    "print(f\"Is close to identity? {np.allclose(identity_check, np.eye(2))}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinant and rank\n",
    "print(\"=== Matrix Determinant and Rank ===\")\n",
    "det = np.linalg.det(square_matrix)\n",
    "rank = np.linalg.matrix_rank(square_matrix)\n",
    "\n",
    "print(f\"Matrix:\")\n",
    "print(square_matrix)\n",
    "print(f\"Determinant: {det:.3f}\")\n",
    "print(f\"Rank: {rank}\")\n",
    "print(f\"Is invertible? {det != 0}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenvalues and eigenvectors\n",
    "print(\"=== Eigenvalues and Eigenvectors ===\")\n",
    "eigenvals, eigenvecs = np.linalg.eig(square_matrix)\n",
    "\n",
    "print(f\"Matrix:\")\n",
    "print(square_matrix)\n",
    "print(f\"\\nEigenvalues: {eigenvals}\")\n",
    "print(f\"\\nEigenvectors:\")\n",
    "print(eigenvecs)\n",
    "\n",
    "# Verify: A * v = λ * v\n",
    "for i, (val, vec) in enumerate(zip(eigenvals, eigenvecs.T)):\n",
    "    left_side = square_matrix @ vec\n",
    "    right_side = val * vec\n",
    "    print(f\"\\nEigenvalue {i+1}: {val:.3f}\")\n",
    "    print(f\"A @ v = {left_side}\")\n",
    "    print(f\"λ * v = {right_side}\")\n",
    "    print(f\"Equal? {np.allclose(left_side, right_side)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singular Value Decomposition (SVD) - Very important for ML!\n",
    "print(\"=== Singular Value Decomposition (SVD) ===\")\n",
    "data_matrix = np.random.randn(5, 3)\n",
    "U, s, Vt = np.linalg.svd(data_matrix)\n",
    "\n",
    "print(f\"Original matrix shape: {data_matrix.shape}\")\n",
    "print(f\"U shape: {U.shape}\")\n",
    "print(f\"Singular values: {s}\")\n",
    "print(f\"V^T shape: {Vt.shape}\")\n",
    "\n",
    "# Reconstruct matrix\n",
    "S = np.zeros(data_matrix.shape)\n",
    "S[:min(data_matrix.shape), :min(data_matrix.shape)] = np.diag(s)\n",
    "reconstructed = U @ S @ Vt\n",
    "\n",
    "print(f\"\\nReconstruction error: {np.linalg.norm(data_matrix - reconstructed):.10f}\")\n",
    "print(f\"Matrices are equal? {np.allclose(data_matrix, reconstructed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: ML Applications - Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement linear regression using matrix operations\n",
    "print(\"=== Linear Regression with Matrices ===\")\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 2\n",
    "\n",
    "# True parameters\n",
    "true_weights = np.array([3.5, -2.1])\n",
    "true_bias = 1.5\n",
    "\n",
    "# Generate features (X) and target (y)\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "y = X @ true_weights + true_bias + 0.1 * np.random.randn(n_samples)\n",
    "\n",
    "print(f\"Data shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"True bias: {true_bias}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve linear regression using normal equation: θ = (X^T X)^(-1) X^T y\n",
    "# Add bias column to X\n",
    "X_with_bias = np.column_stack([np.ones(n_samples), X])\n",
    "print(f\"X with bias column shape: {X_with_bias.shape}\")\n",
    "\n",
    "# Normal equation solution\n",
    "XtX = X_with_bias.T @ X_with_bias\n",
    "Xty = X_with_bias.T @ y\n",
    "theta = np.linalg.inv(XtX) @ Xty\n",
    "\n",
    "estimated_bias = theta[0]\n",
    "estimated_weights = theta[1:]\n",
    "\n",
    "print(f\"Estimated bias: {estimated_bias:.3f} (true: {true_bias:.3f})\")\n",
    "print(f\"Estimated weights: {estimated_weights} (true: {true_weights})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions and error\n",
    "y_pred = X_with_bias @ theta\n",
    "mse = np.mean((y - y_pred) ** 2)\n",
    "r2 = 1 - np.sum((y - y_pred) ** 2) / np.sum((y - np.mean(y)) ** 2)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.6f}\")\n",
    "print(f\"R² Score: {r2:.6f}\")\n",
    "\n",
    "# Visualize results (for first feature)\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], y, alpha=0.6, label='Data')\n",
    "plt.scatter(X[:, 0], y_pred, alpha=0.6, label='Predictions')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "plt.title('Linear Regression Results')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y, y_pred, alpha=0.6)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title(f'Predictions vs True Values (R² = {r2:.3f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: ML Applications - Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement PCA using matrix operations\n",
    "print(\"=== Principal Component Analysis (PCA) ===\")\n",
    "\n",
    "# Generate 2D data with correlation\n",
    "np.random.seed(42)\n",
    "n_points = 200\n",
    "\n",
    "# Create correlated data\n",
    "mean = [0, 0]\n",
    "cov = [[3, 1.5], [1.5, 1]]  # Covariance matrix\n",
    "data_2d = np.random.multivariate_normal(mean, cov, n_points)\n",
    "\n",
    "print(f\"Data shape: {data_2d.shape}\")\n",
    "print(f\"Data mean: {np.mean(data_2d, axis=0)}\")\n",
    "print(f\"Data covariance:\")\n",
    "print(np.cov(data_2d.T))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Center the data (subtract mean)\n",
    "data_centered = data_2d - np.mean(data_2d, axis=0)\n",
    "\n",
    "# Step 2: Calculate covariance matrix\n",
    "cov_matrix = np.cov(data_centered.T)\n",
    "print(f\"Covariance matrix:\")\n",
    "print(cov_matrix)\n",
    "\n",
    "# Step 3: Calculate eigenvalues and eigenvectors\n",
    "eigenvals, eigenvecs = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Sort by eigenvalues (descending)\n",
    "idx = np.argsort(eigenvals)[::-1]\n",
    "eigenvals = eigenvals[idx]\n",
    "eigenvecs = eigenvecs[:, idx]\n",
    "\n",
    "print(f\"\\nEigenvalues (variance explained): {eigenvals}\")\n",
    "print(f\"Explained variance ratio: {eigenvals / np.sum(eigenvals)}\")\n",
    "print(f\"\\nPrincipal components (eigenvectors):\")\n",
    "print(eigenvecs)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Transform data to principal component space\n",
    "data_pca = data_centered @ eigenvecs\n",
    "\n",
    "print(f\"Transformed data shape: {data_pca.shape}\")\n",
    "print(f\"PC1 variance: {np.var(data_pca[:, 0]):.3f}\")\n",
    "print(f\"PC2 variance: {np.var(data_pca[:, 1]):.3f}\")\n",
    "print(f\"Correlation between PCs: {np.corrcoef(data_pca.T)[0, 1]:.6f}\")\n",
    "\n",
    "# Visualize PCA\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original data\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(data_2d[:, 0], data_2d[:, 1], alpha=0.6)\n",
    "plt.title('Original Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "\n",
    "# Data with principal components\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(data_centered[:, 0], data_centered[:, 1], alpha=0.6)\n",
    "# Plot principal component directions\n",
    "origin = np.mean(data_centered, axis=0)\n",
    "for i, (val, vec) in enumerate(zip(eigenvals, eigenvecs.T)):\n",
    "    plt.arrow(origin[0], origin[1], vec[0]*np.sqrt(val)*2, vec[1]*np.sqrt(val)*2, \n",
    "              head_width=0.2, head_length=0.2, fc=f'C{i}', ec=f'C{i}', \n",
    "              label=f'PC{i+1} (λ={val:.2f})')\n",
    "plt.title('Centered Data with PCs')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "\n",
    "# Transformed data (PCA space)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(data_pca[:, 0], data_pca[:, 1], alpha=0.6)\n",
    "plt.title('Data in PCA Space')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Practical Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Matrix Calculator Class\n",
    "Create a comprehensive matrix calculator for ML operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixCalculator:\n",
    "    \"\"\"A comprehensive matrix calculator for ML operations\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def multiply(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Matrix multiplication with dimension checking\"\"\"\n",
    "        # TODO: Implement matrix multiplication with proper error checking\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def transpose(A: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Matrix transpose\"\"\"\n",
    "        # TODO: Implement transpose\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def inverse(A: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Matrix inverse with singularity checking\"\"\"\n",
    "        # TODO: Implement inverse with proper error handling\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def determinant(A: np.ndarray) -> float:\n",
    "        \"\"\"Calculate determinant\"\"\"\n",
    "        # TODO: Implement determinant calculation\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def eigenvalues(A: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Calculate eigenvalues and eigenvectors\"\"\"\n",
    "        # TODO: Implement eigenvalue decomposition\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def svd(A: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Singular Value Decomposition\"\"\"\n",
    "        # TODO: Implement SVD\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def solve_linear_system(A: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Solve Ax = b\"\"\"\n",
    "        # TODO: Implement linear system solver\n",
    "        pass\n",
    "\n",
    "# Test your implementation\n",
    "test_A = np.array([[4, 2], [3, 1]])\n",
    "test_B = np.array([[1, 3], [2, 4]])\n",
    "test_b = np.array([10, 7])\n",
    "\n",
    "# Uncomment these lines when you implement the methods\n",
    "# print(f\"Matrix multiplication: \\n{MatrixCalculator.multiply(test_A, test_B)}\")\n",
    "# print(f\"Determinant: {MatrixCalculator.determinant(test_A)}\")\n",
    "# print(f\"Linear system solution: {MatrixCalculator.solve_linear_system(test_A, test_b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Image Processing with Matrices\n",
    "Apply matrix operations to image processing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_image(size: int = 50) -> np.ndarray:\n",
    "    \"\"\"Create a synthetic image for processing\"\"\"\n",
    "    x, y = np.meshgrid(np.linspace(-1, 1, size), np.linspace(-1, 1, size))\n",
    "    image = np.exp(-(x**2 + y**2))\n",
    "    noise = 0.1 * np.random.randn(size, size)\n",
    "    return image + noise\n",
    "\n",
    "def apply_filter(image: np.ndarray, filter_kernel: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Apply a filter to an image using convolution (simplified)\"\"\"\n",
    "    # TODO: Implement simple convolution\n",
    "    # Hint: For each pixel, multiply neighborhood by kernel and sum\n",
    "    pass\n",
    "\n",
    "def compress_image_svd(image: np.ndarray, n_components: int) -> np.ndarray:\n",
    "    \"\"\"Compress image using SVD\"\"\"\n",
    "    # TODO: Use SVD to compress image by keeping only top n_components\n",
    "    pass\n",
    "\n",
    "# Create test image\n",
    "test_image = create_synthetic_image()\n",
    "\n",
    "# Define filters\n",
    "blur_kernel = np.ones((3, 3)) / 9  # Average filter\n",
    "edge_kernel = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]])  # Edge detection\n",
    "\n",
    "print(f\"Image shape: {test_image.shape}\")\n",
    "print(f\"Image min/max: {test_image.min():.3f} / {test_image.max():.3f}\")\n",
    "\n",
    "# Visualize original image\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(test_image, cmap='gray')\n",
    "plt.title('Original Image')\n",
    "plt.colorbar()\n",
    "\n",
    "# TODO: Apply filters and show results\n",
    "# filtered_blur = apply_filter(test_image, blur_kernel)\n",
    "# filtered_edge = apply_filter(test_image, edge_kernel)\n",
    "# compressed_image = compress_image_svd(test_image, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Neural Network Layer Implementation\n",
    "Implement a simple neural network layer using matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkLayer:\n",
    "    \"\"\"A simple neural network layer using matrix operations\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, output_size: int):\n",
    "        # TODO: Initialize weights and biases\n",
    "        # Hint: Use small random values for weights, zeros for biases\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = None  # Shape: (input_size, output_size)\n",
    "        self.biases = None   # Shape: (output_size,)\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass: X @ W + b\"\"\"\n",
    "        # TODO: Implement forward pass\n",
    "        # Input X shape: (batch_size, input_size)\n",
    "        # Output shape: (batch_size, output_size)\n",
    "        pass\n",
    "    \n",
    "    def apply_activation(self, Z: np.ndarray, activation: str = 'relu') -> np.ndarray:\n",
    "        \"\"\"Apply activation function\"\"\"\n",
    "        # TODO: Implement ReLU and sigmoid activations\n",
    "        if activation == 'relu':\n",
    "            pass\n",
    "        elif activation == 'sigmoid':\n",
    "            pass\n",
    "        else:\n",
    "            return Z  # Linear activation\n",
    "    \n",
    "    def get_parameters(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Return weights and biases\"\"\"\n",
    "        return self.weights, self.biases\n",
    "\n",
    "# Test the neural network layer\n",
    "batch_size, input_dim, hidden_dim = 32, 10, 5\n",
    "test_input = np.random.randn(batch_size, input_dim)\n",
    "\n",
    "layer = NeuralNetworkLayer(input_dim, hidden_dim)\n",
    "# TODO: Test your implementation\n",
    "# output = layer.forward(test_input)\n",
    "# activated_output = layer.apply_activation(output, 'relu')\n",
    "# print(f\"Input shape: {test_input.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")\n",
    "# print(f\"Activated output shape: {activated_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Data Transformation Pipeline\n",
    "Build a complete data preprocessing pipeline using matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer:\n",
    "    \"\"\"Data preprocessing pipeline using matrix operations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mean_ = None\n",
    "        self.std_ = None\n",
    "        self.pca_components_ = None\n",
    "        self.pca_mean_ = None\n",
    "    \n",
    "    def standardize(self, X: np.ndarray, fit: bool = True) -> np.ndarray:\n",
    "        \"\"\"Standardize features to have mean=0, std=1\"\"\"\n",
    "        # TODO: Implement standardization\n",
    "        pass\n",
    "    \n",
    "    def fit_pca(self, X: np.ndarray, n_components: int = None) -> None:\n",
    "        \"\"\"Fit PCA transformation\"\"\"\n",
    "        # TODO: Implement PCA fitting\n",
    "        pass\n",
    "    \n",
    "    def transform_pca(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply PCA transformation\"\"\"\n",
    "        # TODO: Implement PCA transformation\n",
    "        pass\n",
    "    \n",
    "    def add_polynomial_features(self, X: np.ndarray, degree: int = 2) -> np.ndarray:\n",
    "        \"\"\"Add polynomial features (for degree=2: [x1, x2] -> [1, x1, x2, x1^2, x1*x2, x2^2])\"\"\"\n",
    "        # TODO: Implement polynomial feature generation\n",
    "        pass\n",
    "    \n",
    "    def create_interaction_matrix(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Create matrix of all pairwise feature interactions\"\"\"\n",
    "        # TODO: Create interaction features X_i * X_j for all i,j\n",
    "        pass\n",
    "\n",
    "# Generate test data\n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 100, 5\n",
    "test_data = np.random.randn(n_samples, n_features)\n",
    "test_data[:, 0] *= 10  # Different scales\n",
    "test_data[:, 1] += 5\n",
    "\n",
    "print(f\"Original data shape: {test_data.shape}\")\n",
    "print(f\"Original data mean: {np.mean(test_data, axis=0)}\")\n",
    "print(f\"Original data std: {np.std(test_data, axis=0)}\")\n",
    "\n",
    "transformer = DataTransformer()\n",
    "# TODO: Test your implementation\n",
    "# standardized_data = transformer.standardize(test_data)\n",
    "# transformer.fit_pca(standardized_data, n_components=3)\n",
    "# pca_data = transformer.transform_pca(standardized_data)\n",
    "# poly_data = transformer.add_polynomial_features(test_data[:5, :2])  # Small subset for demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Key Takeaways and Next Steps\n",
    "\n",
    "### What You've Learned:\n",
    "1. ✅ **Matrix Creation**: Multiple methods and data structures\n",
    "2. ✅ **Matrix Operations**: Multiplication, transpose, inverse, decompositions\n",
    "3. ✅ **Linear Algebra**: Eigenvalues, SVD, solving linear systems\n",
    "4. ✅ **ML Applications**: Linear regression, PCA implementation\n",
    "5. ✅ **Advanced Topics**: Neural network layers, data transformations\n",
    "\n",
    "### Essential Matrix Formulas:\n",
    "- **Matrix Multiplication**: C = A @ B (where C[i,j] = Σ A[i,k] × B[k,j])\n",
    "- **Linear Regression**: θ = (X^T X)^(-1) X^T y\n",
    "- **PCA**: Data_pca = (Data - mean) @ eigenvectors\n",
    "- **Neural Layer**: Output = X @ W + b\n",
    "\n",
    "### Matrix Operations Performance Tips:\n",
    "- Use `@` operator for matrix multiplication (clearest syntax)\n",
    "- Vectorize operations instead of loops\n",
    "- Consider numerical stability for inversions\n",
    "- Use SVD for robust decompositions\n",
    "\n",
    "### Next Steps:\n",
    "1. **Complete all exercises** in this notebook\n",
    "2. **Practice with real datasets** (load images, apply transformations)\n",
    "3. **Move to Module 3**: Calculus & Optimization\n",
    "4. **Build projects**: Implement k-means clustering, build a simple neural network\n",
    "\n",
    "### Common ML Applications of Matrices:\n",
    "- **Dataset storage** (rows=samples, columns=features)\n",
    "- **Neural network weights** and computations\n",
    "- **Image processing** and computer vision\n",
    "- **Dimensionality reduction** (PCA, SVD)\n",
    "- **Recommendation systems** (matrix factorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Self-Assessment Quiz\n",
    "\n",
    "Test your understanding with these questions:\n",
    "\n",
    "### Conceptual Questions:\n",
    "1. What's the difference between element-wise multiplication (`*`) and matrix multiplication (`@`)?\n",
    "2. When can you multiply two matrices? What determines the output shape?\n",
    "3. Why is the transpose operation important in linear regression?\n",
    "4. How does PCA use eigenvalues and eigenvectors?\n",
    "5. What happens when a matrix is not invertible?\n",
    "\n",
    "### Practical Questions:\n",
    "1. Calculate the matrix product of [[1,2], [3,4]] and [[5,6], [7,8]]\n",
    "2. What's the shape of the result when multiplying a (100, 5) matrix with a (5, 3) matrix?\n",
    "3. If you have 1000 samples with 10 features each, what's the shape of your data matrix?\n",
    "\n",
    "### Coding Challenges:\n",
    "1. Implement matrix multiplication from scratch using nested loops\n",
    "2. Create a function that checks if a matrix is symmetric\n",
    "3. Implement a simple linear regression class using only matrix operations\n",
    "4. Build a PCA class that can reduce dimensionality of any dataset\n",
    "\n",
    "### Performance Questions:\n",
    "1. Why are vectorized operations faster than loops in Python?\n",
    "2. When would you use SVD instead of eigendecomposition?\n",
    "3. How does matrix size affect computation time for different operations?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}